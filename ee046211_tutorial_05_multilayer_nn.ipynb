{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <img src=\"./assets/course-icon.png\" style=\"height:50px;display:inline\"> ECE 046211 - Technion - Deep Learning\n",
    "---\n",
    "\n",
    "#### <a href=\"https://taldatech.github.io\">Tal Daniel</a>\n",
    "\n",
    "## Tutorial 05 - Multilayer Neural Networks\n",
    "---\n",
    "<center><img src=\"./assets/mlp.jpg\" style=\"height:200px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "\n",
    "* [Multi-Layer Perceptron (MLP)](#-Multi-Layer-Perceptron-(MLP))\n",
    "* [Modular Approach - Autodiff Reverse Mode](#-Modular-Approach---Autodiff-Reverse-Mode)\n",
    "* [Example - Neural Networks for Regression - Housing Prices](#-Example---Neural-Networks-for-Regression---Housing-Prices)\n",
    "* [Building a Neural Network with PyTorch](#-Building-a-Neural-Network-with-PyTorch)\n",
    "* [Weights Initialization](#-Weights-Initialization)\n",
    "* [Neural Network Weight Initialization with PyTorch](#-Neural-Network-Weight-Initialization-with-PyTorch)\n",
    "* [Deep Double Descent](#-Deep-Double-Descent)\n",
    "* [Recommended Videos](#-Recommended-Videos)\n",
    "* [Credits](#-Credits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LedcT50PTD1e",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### <img src=\"./assets/colab_icon.PNG\" style=\"height:30px;display:inline\"> Additional Packeges for Google Colab\n",
    "----\n",
    "If you are using <a href=\"https://colab.research.google.com/\">Google Colab</a>, you have to install additional packages. To do this, simply run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1O-9ASIdTD1e",
    "outputId": "9e4a777b-5537-47ea-abab-5efd32eed6d1",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# to work locally (win/linux/mac), first install 'graphviz': https://graphviz.org/download/ and restart your machine\n",
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# imports for the tutorial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchviz\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/layers.png\" style=\"height:50px;display:inline\"> Multi-Layer Perceptron (MLP)\n",
    "---\n",
    "* An MLP is composed of one input layer, one or more hidden layers and a final output layer. \n",
    "* Every layer, except the output layer, optionally includes a **bias neuron** which is fully connected to the next layer. \n",
    "* When the number of hidden layers is larger than 2, the network is usually called a deep neural network (DNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* MLPs are trained with the *backpropgation* algorithm, which is composed of two main parts: **forward pass and backward pass**.\n",
    "    * This **autodiff reverse mode**.\n",
    "* In the *forward pass*, for each training instance, the algorithm feeds it to the network and computes the output of every neuron in each consecutive layer (using the network for prediction is just doing a forward pass). \n",
    "* Then, the output error (the difference between the desired output and the actual output from the network), which is task dependent, is computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* After the output error calculation, the network calculates how much each neuron in the last hidden layer contributed to the output error (using the **chain rule**).\n",
    "* It then proceeds to measure how much of these error contributions came from each neuron in the previous layers until reaching the input layer. \n",
    "* This is the *backward pass*: measuring the error gradient across all the connection weights in the network by propagating the error gradient backward in the network (this is the backpropagation process)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In short: for each training instance (=batch) the **backpropagation algorithm** first makes a prediction (forward pass), measures the error, then goes in reverse to measure the error contribution from each connection (backward pass) and finally, using Gradient Descent, updates the weights in the direction that reduces the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./assets/mlp_anim.gif\" style=\"height:300px\"></center>\n",
    "\n",
    "<a href=\"https://medium.com/the-feynman-journal/the-linear-and-nonlinear-nature-of-feedforward-84199eb3edea\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"./assets/mlp_example.jpg\" style=\"height:300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, if: $$ X \\in \\mathbb{R}^2 $$ $$ W_1 \\in \\mathbb{R}^{2 \\times 4} $$ $$ W_2 \\in \\mathbb{R}^{4 \\times 3} $$ $$ W_3 \\in \\mathbb{R}^{3 \\times 1} $$ $$ b_1 \\in \\mathbb{R}^4 $$ $$ b_2 \\in \\mathbb{R}^3 $$ $$ b_3 \\in \\mathbb{R} $$ Then: $$ F(X,W) = W_3^T \\phi_2(W_2^T\\phi_1(W_1^TX + b_1) + b_2) + b_3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The key change made to the Perceptron that brought upon the era of deep learning is the addition of **activation function** to the output of each neuron. These allow the learning of non-linear functions. Some popular activation functions:\n",
    "1. **Logistic function (sigmoid)**: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$. The output is in $[0,1]$ which can be used for binary classification or as a probability. In PyTorch: <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html\">`nn.Sigmoid()` or `torch.sigmoid()`</a>.\n",
    "2. **Hyperbolic tangent function**: $tanh(z) = 2\\sigma(2z) - 1$. The output is in $[-1,1]$ which tends to make each layer's output more or less normalized at the beginning of the training (which may speed up convergence). In PyTorch: <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html\">`nn.Tanh()` or `torch.tanh()`</a>.\n",
    "3. **ReLU (Rectified Linear Unit) function**: $ReLU(z) = \\max(0,z)$. Continuous but not differentiable at $z=0$. However, it is the most common activation function as it is fast to compute and does not bound the output (which helps with some issues during Gradient Descent). In PyTorch: <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\">`nn.ReLU()` or `torch.relu()`</a>.\n",
    "4. **LeakyReLU function**: $LeakyReLU(z) = \\max(0,x) + \\text{negative-slope} * \\min(0,x)$. An activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is a hyper-parameter determined before training. This type of activation function is popular in tasks where we we may suffer from sparse gradients. In PyTorch: <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html\">`nn.LeakyReLU(negative_slope=0.01)`</a>.\n",
    "    * Also see **Exponential Linear Unit (ELU)**: In PyTorch: <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.ELU.html\">`nn.ELU(alpha=1.0)`</a>.\n",
    "5. **Gaussian Error Linear Units (GELU) function**: $GELU(x) = x * \\Phi(x)$, where $\\Phi(x)$ is the Cumulative Distribution Function (CDF) for the standard Gaussian Distribution. In PyTorch: <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.GELU.html\">`nn.GELU()`</a>. <a href=\"https://paperswithcode.com/method/gelu\">Read More</a>.\n",
    "6. **Sigmoid Linear Unit (SiLU) function**: $silu(x) = x * \\sigma(x)$. In PyTorch: <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html\">`nn.SiLU()`</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_mlp_activations.PNG\" style=\"height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/lego-head.png\" style=\"height:50px;display:inline\"> Modular Approach - Autodiff Reverse Mode\n",
    "---\n",
    "* We code **layers**, not networks.\n",
    "* Layer Specification - each layer needs to provide 3 functions:\n",
    "    1. The layer output given its input (forward pass) $Z^{(k+1)} = f(Z^{(k)}) $.\n",
    "    2. Derivative with respect to the input $  \\frac{\\partial Z^{(k+1)}}{\\partial Z^{(k)}} $.\n",
    "    3. Derivative with respect to parameters $  \\frac{\\partial Z^{(k+1)}}{\\partial W^{(k)}} $.\n",
    "       \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Illustration: <center><img src=\"./assets/modular_approach_1.jpg\" style=\"height:350px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Zoom-in: <center><img src=\"./assets/modular_approach_2.jpg\" style=\"height:200px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./assets/local_gradient.png\" style=\"height:200px\"></center>\n",
    "\n",
    "Image source: CS231n Lecture 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/plasticine/100/000000/serial-tasks.png\" style=\"height:50px;display:inline\"> Backpropagation\n",
    "---\n",
    "We now establish a common language when it comes to neural networks architecture (assume single dimension):\n",
    "* **Forward Pass**: $Z^{(k+1)} = f(Z^{(k)}) $\n",
    "* **Backward Pass**: $\\delta^{(k+1)} = \\frac{\\partial E}{\\partial Z^{(k+1)}}$\n",
    "* Applying the **chain rule** for a single layer: $$ \\frac{\\partial E}{\\partial Z^{(k)}} = \\frac{\\partial E}{\\partial Z^{(k+1)}} \\frac{\\partial Z^{(k+1)}}{\\partial Z^{(k)}} = \\delta^{(k+1)}\\frac{\\partial Z^{(k+1)}}{\\partial Z^{(k)}} = \\delta^{(k+1)}\\frac{\\partial f(Z^{(k)})}{\\partial Z^{(k)}} $$\n",
    "* The **gradient with respect to layer parameters** (if it has any): $$ \\frac{\\partial E}{\\partial W^{(k)}} = \\frac{\\partial E}{\\partial Z^{(k+1)}} \\frac{\\partial Z^{(k+1)}}{\\partial W^{(k)}} = \\delta^{(k+1)} \\frac{\\partial Z^{(k+1)}}{\\partial W^{(k)}}  $$\n",
    "\n",
    "* **Important Note**: in the above, for multi-dimensional tensors, there is an abuse of dimensions above: $ \\frac{\\partial Z^{(k+1)}}{\\partial W^{(k)}} $ is not the proper way of getting the derivative of \"a vector w.r.t. a matrix\". See explanation under the **The Linear Layer** section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/96/000000/3d-glasses.png\" style=\"height:50px;display:inline\"> Extension to Multi-Dimensions\n",
    "---\n",
    "* $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ is a vector function of a vector variable: $$ f(x) = \\begin{bmatrix} f_1(x) \\\\ \\vdots \\\\ f_m(x) \\end{bmatrix}, x \\in \\mathbb{R}^n, f(x) \\in \\mathbb{R}^m $$\n",
    "* The **gradient** is given by: $$ \\frac{\\partial f_i}{\\partial x} = \\big[ \\frac{\\partial f_i(x)}{\\partial x_1}, ..., \\frac{\\partial f_i(x)}{\\partial x_n} \\big] $$\n",
    "* The **Jacobian**, $J_f(x) \\in \\mathbb{R}^{m \\times n}$, is given by: $$ J_f(x) = \\begin{bmatrix} \\frac{\\partial f_1(x)}{\\partial x}\\\\ \\vdots\\\\ \\frac{\\partial f_m(x)}{\\partial x} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} && \\cdots && \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots && \\ddots && \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} && \\cdots && \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **The Chain Rule**:\n",
    "    * Given: $$ F: \\mathbb{R}^n \\to \\mathbb{R}^m $$ $$ \\phi: \\mathbb{R}^m \\to \\mathbb{R}^k $$ $$ \\psi(x)= \\phi(F(x)) $$\n",
    "    * The Jacobian is given by: $$ J_{\\psi} = J_{\\phi} J_F $$ $$ J_{\\phi} \\in \\mathbb{R}^{k \\times m}, J_F \\in  \\mathbb{R}^{m \\times n} \\to J_{\\psi} \\in \\mathbb{R}^{k \\times n}$$\n",
    "    * Recall that `autograd` implements reverse mode Autodiff as a vector-Jacobian multiplication engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"./assets/backprop_anim.gif\" style=\"height:350px\"></center>\n",
    "<a href=\"https://anatomiesofintelligence.github.io/posts/2018-10-16-forward-back-propogation\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/popular-topic.png\" style=\"height:50px;display:inline\"> Commonly Used Layers (as Modular Blocks)\n",
    "---\n",
    "* Linear Layer (linear combination of the inputs).\n",
    "* Activation Layer (usually together with the linear layer, apply a function on the linear combination of weighted inputs): ReLU, Binary Step, Sigmoid, TanH and etc...\n",
    "* Softmax Layer (Sigmoid for more than 2 classes, outputs the probability of each class) for classification tasks.\n",
    "* Loss Function Layer (e.g., MSE and Cross Entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/home.png\" style=\"height:50px;display:inline\"> Example - Neural Networks for Regression - Housing Prices\n",
    "---\n",
    "* The Housing Prices Dataset:\n",
    "    * Two input features: *Size* and *Floor*\n",
    "    * One output: *House Price*\n",
    "    * **Loss function**: MSE\n",
    "* Suggested **network architecture**: 2 hidden layers\n",
    "    * Two inputs, one for each feature\n",
    "    * Four neurons in the *first hidden layer*\n",
    "    * Three neurons in the *second hidden layer*\n",
    "    * One output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Layout: <center><img src=\"./assets/mlp_example.jpg\" style=\"height:300px\"></center> $$ F(X,W) = W_3^T \\phi_2(W_2^T\\phi_1(W_1^TX + b_1) + b_2) + b_3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Where: $$ X \\in \\mathbb{R}^2 $$ $$ W_1 \\in \\mathbb{R}^{2 \\times 4} $$ $$ W_2 \\in \\mathbb{R}^{4 \\times 3} $$ $$ W_3 \\in \\mathbb{R}^{3 \\times 1} $$ $$ b_1 \\in \\mathbb{R}^4 $$ $$ b_2 \\in \\mathbb{R}^3 $$ $$ b_3 \\in \\mathbb{R} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/office/80/000000/baby-footprints-path.png\" style=\"height:50px;display:inline\"> Step-by-Step Solution\n",
    "---\n",
    "* The MSE loss function over all the training examples $x_i$ and the corresponding training targets: $$ Error = \\frac{1}{N} \\sum_{i=1}^N (F(x_i, W) - y_i)^2 = \\frac{1}{N} ||F(X, W) - Y||_2^2 $$\n",
    "* Linear Layer: $$ u_{out} = W^Tu_{in} + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Activation Layer:\n",
    "    * $\\phi_1$ and $\\phi_2$ are multivariate vector *nonlinear* functions, such that: $$ \\phi(U) = \\phi\\left(\\begin{bmatrix} u_1 \\\\ \\vdots \\\\ u_n \\end{bmatrix}\\right) = \\begin{bmatrix} \\phi(u_1) \\\\ \\vdots \\\\ \\phi(u_n) \\end{bmatrix} $$\n",
    "    * For **ReLU**: $$ \\begin{bmatrix} \\phi(u_1) \\\\ \\vdots \\\\ \\phi(u_n) \\end{bmatrix} = \\begin{bmatrix} \\max(0, u_1) \\\\ \\vdots \\\\ \\max(0, u_n) \\end{bmatrix} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/color/96/000000/layers.png\" style=\"height:20px;display:inline\"> The Linear Layer\n",
    "---\n",
    "* **Forward Pass**: $$Z^{(k+1)} = f(Z^{(k)}) = (W^{(k)})^T Z^{(k)} + b^{(k)}.$$\n",
    "    * $k$ denotes the $k^{th}$ layer with the corresponding weights and bias $W^{(k)}, b^{(k)}$.\n",
    "* **Derivative** with respect to *input* $Z^{(k)}$: $$ \\frac{\\partial Z^{(k+1)}}{\\partial Z^{(k)}} = \\frac{\\partial ((W^{(k)})^T Z^{(k)} + b^{(k)})}{\\partial Z^{(k)}} = (W^{(k)})^T, $$ <br> $$ \\delta^{(k)} = \\delta^{(k+1)} (W^{(k)})^T . $$\n",
    "* **Derivative** with respect to the *parameters* $W^{(k)}, b^{(k)}$: $$ \\frac{\\partial Z^{(k+1)}}{\\partial W^{(k)}} = Z^{(k)} ,\\frac{\\partial E}{\\partial W^{(k)}} = Z^{(k)} \\delta^{(k+1)^T},  $$ <br> $$  \\frac{\\partial Z^{(k+1)}}{\\partial b^{(k)}} = I,  \\frac{\\partial E}{\\partial b^{(k)}} = \\delta^{(k+1)}.$$\n",
    "\n",
    "<br>\n",
    "\n",
    "* **Important Note**: in the above, we *abused* the dimensions, e.g., $ \\frac{\\partial Z^{(k+1)}}{\\partial W^{(k)}} = Z^{(k)} $ is not the proper way of getting the derivative of \"a vector w.r.t. a matrix\". The proper way of doing this calculation uses <a href=\"https://en.wikipedia.org/wiki/Kronecker_product\">Kronecker product</a>: $ \\frac{\\partial(Ax)}{\\partial A} = x \\otimes I_m$, where $I_m$ is the $m \\times m$ identity matrix. However, since we are starting our calculation from a scalar (our loss), the computation is simplified to a simple *outer product*: $\\frac{\\partial E}{\\partial W^{(k)}} = Z^{(k)} \\delta^{(k+1)^T}  $.\n",
    "    * <a href=\"https://math.stackexchange.com/questions/1621948/derivative-of-a-vector-with-respect-to-a-matrix\">\"Derivative of a vector with respect to a matrix\" @ math.stackexchange.com</a>.\n",
    "    * <a href=\"http://cs231n.stanford.edu/vecDerivs.pdf\">Vector, Matrix, and Tensor Derivatives - Erik Learned-Miller - page 4</a>.\n",
    "\n",
    "* <a style='color:red'>**Tip**</a>: when calculating backpropagation by-hand, it is always good to perform calculations parameter-wise (i.e., calculate the gradient per parameter and not per layer) to avoid dimensionality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####  <img src=\"https://img.icons8.com/color/96/000000/layers.png\" style=\"height:20px;display:inline\">  The ReLU Layer\n",
    "---\n",
    "* **Forward Pass**: $$ Z^{(k+1)} = \\begin{bmatrix} max(0,Z_1^{(k)}) \\\\ \\vdots \\\\ max(0,Z_n^{(k)}) \\end{bmatrix}, ReLU(Z): \\mathbb{R}^{n} \\to \\mathbb{R}^n $$\n",
    "* **Derivative** with respect to *input* $Z^{(k)}$: $$ \\phi = max(0,Z^{(k)}), \\phi' = heaviside(Z^{(k)}) $$ <br> $$ \\frac{\\partial Z^{(k+1)}}{\\partial Z^{(k)}} = diag(\\phi') $$ <br> $$ \\delta^{(k)} = \\delta^{(k+1)} diag (\\phi') $$\n",
    "* **Derivative** with respect to the *parameters*: **NO PARAMETERS!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####  <img src=\"https://img.icons8.com/color/96/000000/layers.png\" style=\"height:20px;display:inline\">  The MSE Layer \n",
    "---\n",
    "* **Forward Pass**: $$ E = Z^{(k+1)} = ||Z^{(k)} - y||^2 $$\n",
    "    * For simplicity, we omit the $\\frac{1}{N}$ factor before the MSE term.\n",
    "*  **Derivative** with respect to *input* $Z^{(k)}$: $$ \\delta^{(k+1)} = \\frac{\\partial E}{\\partial Z^{(k+1)}} = \\frac{\\partial E}{\\partial E} = 1 $$ <br> $$ \\frac{\\partial Z^{(k+1)}}{\\partial Z^{(k)}} = 2(Z^{(k)} - y) $$ <br> $$ \\delta^{(k)} = \\delta^{(k+1)} 2(Z^{(k)} - y) =  2(Z^{(k)} - y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/fast-forward.png\" style=\"height:50px;display:inline\"> Forward Pass\n",
    "---\n",
    "$$ F(X,W) = W_3^T \\phi_2(W_2^T\\phi_1(W_1^TX + b_1) + b_2) + b_3 $$\n",
    "<center><img src=\"./assets/forward_pass.JPG\" style=\"height:300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/rewind.png\" style=\"height:50px;display:inline\"> Backward Pass\n",
    "---\n",
    "The following illustration depicts the backpropagation process:\n",
    "\n",
    "<center><img src=\"./assets/backward_pass.JPG\" style=\"height:400px\"></center>\n",
    "\n",
    "* Remember that `autograd` is a vector-Jacobian multiplication engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/olympic-torch.png\" style=\"height:50px;display:inline\"> Building a Neural Network with PyTorch\n",
    "---\n",
    "We will now implement a neural network for regression with PyTorch. We will use the \"Boston House Prices\" dataset and use the architecture described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define our neural network model\n",
    "# this approach provides easier access to weights (e.g., 'model.fc1' will return the parameters of the first layer)\n",
    "class HousePricesMLP(nn.Module):\n",
    "    # notice that we inherit from nn.Module\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(HousePricesMLP, self).__init__()\n",
    "        # here we initialize the building blocks of our network\n",
    "        # single neuron is just one linear (fully-connected) layer\n",
    "        self.fc_1 = nn.Linear(input_dim, 4) \n",
    "        self.fc_2 = nn.Linear(4, 3)\n",
    "        self.output_layer = nn.Linear(3, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # here we define what happens to the input x in the forward pass\n",
    "        # that is, the order in which x goes through the building blocks\n",
    "        x = torch.relu(self.fc_1(x))\n",
    "        x = torch.relu(self.fc_2(x))\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# alternative method - more readdable, easier to code, less convenient access to weights\n",
    "# e.g., to access the first layer weights -- `model.hidden[0]`\n",
    "class HousePricesMLP(nn.Module):\n",
    "    # notice that we inherit from nn.Module\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(HousePricesMLP, self).__init__()\n",
    "        # here we initialize the building blocks of our network\n",
    "        # single neuron is just one linear (fully-connected) layer\n",
    "        self.hidden = nn.Sequential(nn.Linear(input_dim, 4),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(4, 3),\n",
    "                                    nn.ReLU())\n",
    "        self.output_layer = nn.Linear(3, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # here we define what happens to the input x in the forward pass\n",
    "        # that is, the order in which x goes through the building blocks\n",
    "        return self.output_layer(self.hidden(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: in this example we are using a very simple NN model\n",
    "# We usually wider and deeper networks such as this one:\n",
    "class HousePricesMLP(nn.Module):\n",
    "    # notice that we inherit from nn.Module\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
    "        super(HousePricesMLP, self).__init__()\n",
    "        # here we initialize the building blocks of our network\n",
    "        # single neuron is just one linear (fully-connected) layer\n",
    "        self.hidden = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                                    nn.ReLU(),)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # here we define what happens to the input x in the forward pass\n",
    "        # that is, the order in which x goes through the building blocks\n",
    "        return self.output_layer(self.hidden(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data and preprocess\n",
    "boston_dataset = load_boston()\n",
    "# print description of the features\n",
    "print(boston_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.22489</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.377</td>\n",
       "      <td>94.3</td>\n",
       "      <td>6.3467</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>392.52</td>\n",
       "      <td>20.45</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>391.70</td>\n",
       "      <td>9.71</td>\n",
       "      <td>26.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>12.04820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>5.648</td>\n",
       "      <td>87.6</td>\n",
       "      <td>1.9512</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>291.55</td>\n",
       "      <td>14.10</td>\n",
       "      <td>20.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.05372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.437</td>\n",
       "      <td>6.549</td>\n",
       "      <td>51.0</td>\n",
       "      <td>5.9604</td>\n",
       "      <td>4.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>392.85</td>\n",
       "      <td>7.39</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>0.07950</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.411</td>\n",
       "      <td>6.579</td>\n",
       "      <td>35.9</td>\n",
       "      <td>10.7103</td>\n",
       "      <td>4.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>18.3</td>\n",
       "      <td>370.78</td>\n",
       "      <td>5.49</td>\n",
       "      <td>24.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.53412</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.647</td>\n",
       "      <td>7.520</td>\n",
       "      <td>89.4</td>\n",
       "      <td>2.1398</td>\n",
       "      <td>5.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>388.37</td>\n",
       "      <td>7.26</td>\n",
       "      <td>43.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>0.01870</td>\n",
       "      <td>85.0</td>\n",
       "      <td>4.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.429</td>\n",
       "      <td>6.516</td>\n",
       "      <td>27.7</td>\n",
       "      <td>8.5353</td>\n",
       "      <td>4.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>392.43</td>\n",
       "      <td>6.36</td>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.06664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510</td>\n",
       "      <td>6.546</td>\n",
       "      <td>33.1</td>\n",
       "      <td>3.1323</td>\n",
       "      <td>5.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>390.96</td>\n",
       "      <td>5.33</td>\n",
       "      <td>29.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>10.83420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.679</td>\n",
       "      <td>6.782</td>\n",
       "      <td>90.8</td>\n",
       "      <td>1.8195</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>21.57</td>\n",
       "      <td>25.79</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM   AGE      DIS   RAD    TAX  \\\n",
       "10    0.22489  12.5   7.87   0.0  0.524  6.377  94.3   6.3467   5.0  311.0   \n",
       "220   0.35809   0.0   6.20   1.0  0.507  6.951  88.5   2.8617   8.0  307.0   \n",
       "422  12.04820   0.0  18.10   0.0  0.614  5.648  87.6   1.9512  24.0  666.0   \n",
       "296   0.05372   0.0  13.92   0.0  0.437  6.549  51.0   5.9604   4.0  289.0   \n",
       "1     0.02731   0.0   7.07   0.0  0.469  6.421  78.9   4.9671   2.0  242.0   \n",
       "351   0.07950  60.0   1.69   0.0  0.411  6.579  35.9  10.7103   4.0  411.0   \n",
       "261   0.53412  20.0   3.97   0.0  0.647  7.520  89.4   2.1398   5.0  264.0   \n",
       "347   0.01870  85.0   4.15   0.0  0.429  6.516  27.7   8.5353   4.0  351.0   \n",
       "175   0.06664   0.0   4.05   0.0  0.510  6.546  33.1   3.1323   5.0  296.0   \n",
       "416  10.83420   0.0  18.10   0.0  0.679  6.782  90.8   1.8195  24.0  666.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  MEDV  \n",
       "10      15.2  392.52  20.45  15.0  \n",
       "220     17.4  391.70   9.71  26.7  \n",
       "422     20.2  291.55  14.10  20.8  \n",
       "296     16.0  392.85   7.39  27.1  \n",
       "1       17.8  396.90   9.14  21.6  \n",
       "351     18.3  370.78   5.49  24.1  \n",
       "261     13.0  388.37   7.26  43.1  \n",
       "347     17.9  392.43   6.36  23.1  \n",
       "175     16.6  390.96   5.33  29.4  \n",
       "416     20.2   21.57  25.79   7.5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the target is the MEDV field - median value of owner-occupied homes in 1000$\n",
    "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "boston['MEDV'] = boston_dataset.target\n",
    "boston.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training samples: 404, total test samples: 102\n"
     ]
    }
   ],
   "source": [
    "# we will use 2 features\n",
    "x = boston[['RM', 'LSTAT']].values  # RM-num rooms, LSTAT-% lower status of the population\n",
    "y = boston['MEDV'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=5)\n",
    "# scaling\n",
    "x_scaler = StandardScaler()\n",
    "x_scaler.fit(x_train)\n",
    "x_train = x_scaler.transform(x_train)\n",
    "x_test = x_scaler.transform(x_test)\n",
    "print(\"total training samples: {}, total test samples: {}\".format(len(x_train),len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: features: tensor([-0.8488,  0.8353]), target: 13.100000381469727\n"
     ]
    }
   ],
   "source": [
    "# convert to tensor dataset for PyTorch\n",
    "# boston_tensor_train_ds = TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).float()) # old method\n",
    "boston_tensor_train_ds = TensorDataset(torch.tensor(x_train, dtype=torch.float), torch.tensor(y_train, dtype=torch.float))\n",
    "# check\n",
    "print(f'sample 0: features: {boston_tensor_train_ds[0][0]}, target: {boston_tensor_train_ds[0][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define hyper-parmeters and create our model\n",
    "num_features = 2\n",
    "output_dim = 1\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "num_epochs = 500\n",
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# loss criterion\n",
    "criterion = nn.MSELoss()\n",
    "# model\n",
    "model = HousePricesMLP(num_features, output_dim).to(device)\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 611.3372344970703\n",
      "epoch: 50 loss: 26.573792934417725\n",
      "epoch: 100 loss: 23.42008924484253\n",
      "epoch: 150 loss: 22.77083158493042\n",
      "epoch: 200 loss: 22.498516082763672\n",
      "epoch: 250 loss: 22.412588596343994\n",
      "epoch: 300 loss: 22.351622104644775\n",
      "epoch: 350 loss: 22.29261350631714\n",
      "epoch: 400 loss: 22.230255603790283\n",
      "epoch: 450 loss: 22.17220449447632\n"
     ]
    }
   ],
   "source": [
    "boston_tensor_train_dataloader = DataLoader(boston_tensor_train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# training loop for the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # change the mode to training, activating layers like DropOut and BatchNorm, if there are any\n",
    "    epoch_losses = []\n",
    "    for features, targets in boston_tensor_train_dataloader:\n",
    "        # send data to device\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # forward pass\n",
    "        output = model(features)  # calls model.forward(features)\n",
    "        # loss\n",
    "        loss = criterion(output.view(-1), targets)\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()  # clean the gradients from previous iteration, clears the `tensor.grad` field (tensor.grad=0)\n",
    "        loss.backward()  # autograd backward to calculate gradients, assigns the `tensor.grad` field (e.g., tensor.grad=0.27)\n",
    "        optimizer.step()  # apply update to the weights, applies the gradient update rule of the optimizer (param=param - lr * grad)\n",
    "        epoch_losses.append(loss.item())\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'epoch: {epoch} loss: {np.mean(epoch_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test MSE error: 15.394033432006836\n"
     ]
    }
   ],
   "source": [
    "# test error\n",
    "model.eval()  # put the model in evaluation mode, turns-off drop-out, changes functionality of BatchNorm\n",
    "# use model.train() to change back to traininig mode\n",
    "with torch.no_grad():\n",
    "    # set requires_grad=False for all tensors (weights and biases)\n",
    "    # test_outputs = model(torch.from_numpy(x_test).float().to(device))  # old method\n",
    "    test_outputs = model(torch.tensor(x_test, dtype=torch.float, device=device))\n",
    "    test_error = criterion(test_outputs.view(-1), torch.tensor(y_test, dtype=torch.float, device=device))\n",
    "print(f'test MSE error: {test_error.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.46.0 (20210118.1747)\r\n",
       " -->\r\n",
       "<!-- Pages: 1 -->\r\n",
       "<svg width=\"374pt\" height=\"371pt\"\r\n",
       " viewBox=\"0.00 0.00 374.00 371.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 367)\">\r\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-367 370,-367 370,4 -4,4\"/>\r\n",
       "<!-- 2419740177632 -->\r\n",
       "<g id=\"node1\" class=\"node\">\r\n",
       "<title>2419740177632</title>\r\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"223.5,-21 122.5,-21 122.5,0 223.5,0 223.5,-21\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"173\" y=\"-7.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">AddmmBackward</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740180208 -->\r\n",
       "<g id=\"node2\" class=\"node\">\r\n",
       "<title>2419740180208</title>\r\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"110,-78 0,-78 0,-57 110,-57 110,-78\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"55\" y=\"-64.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">output_layer.bias (1)</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740180208&#45;&gt;2419740177632 -->\r\n",
       "<g id=\"edge1\" class=\"edge\">\r\n",
       "<title>2419740180208&#45;&gt;2419740177632</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M75.55,-56.92C94.22,-48.22 121.98,-35.28 143.1,-25.43\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"144.7,-28.55 152.29,-21.16 141.74,-22.21 144.7,-28.55\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740177688 -->\r\n",
       "<g id=\"node3\" class=\"node\">\r\n",
       "<title>2419740177688</title>\r\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"218,-78 128,-78 128,-57 218,-57 218,-78\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"173\" y=\"-64.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">ReluBackward0</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740177688&#45;&gt;2419740177632 -->\r\n",
       "<g id=\"edge2\" class=\"edge\">\r\n",
       "<title>2419740177688&#45;&gt;2419740177632</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M173,-56.92C173,-49.91 173,-40.14 173,-31.47\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"176.5,-31.34 173,-21.34 169.5,-31.34 176.5,-31.34\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740178304 -->\r\n",
       "<g id=\"node4\" class=\"node\">\r\n",
       "<title>2419740178304</title>\r\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"213.5,-135 112.5,-135 112.5,-114 213.5,-114 213.5,-135\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"163\" y=\"-121.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">AddmmBackward</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740178304&#45;&gt;2419740177688 -->\r\n",
       "<g id=\"edge3\" class=\"edge\">\r\n",
       "<title>2419740178304&#45;&gt;2419740177688</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M164.74,-113.92C166.02,-106.91 167.79,-97.14 169.37,-88.47\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"172.87,-88.81 171.21,-78.34 165.98,-87.55 172.87,-88.81\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740180152 -->\r\n",
       "<g id=\"node5\" class=\"node\">\r\n",
       "<title>2419740180152</title>\r\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"99.5,-192 6.5,-192 6.5,-171 99.5,-171 99.5,-192\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"53\" y=\"-178.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">hidden.2.bias (3)</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740180152&#45;&gt;2419740178304 -->\r\n",
       "<g id=\"edge4\" class=\"edge\">\r\n",
       "<title>2419740180152&#45;&gt;2419740178304</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M72.16,-170.92C89.41,-162.3 114.98,-149.51 134.6,-139.7\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"136.31,-142.76 143.69,-135.16 133.18,-136.5 136.31,-142.76\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740178360 -->\r\n",
       "<g id=\"node6\" class=\"node\">\r\n",
       "<title>2419740178360</title>\r\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-192 118,-192 118,-171 208,-171 208,-192\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"163\" y=\"-178.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">ReluBackward0</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740178360&#45;&gt;2419740178304 -->\r\n",
       "<g id=\"edge5\" class=\"edge\">\r\n",
       "<title>2419740178360&#45;&gt;2419740178304</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163,-170.92C163,-163.91 163,-154.14 163,-145.47\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"166.5,-145.34 163,-135.34 159.5,-145.34 166.5,-145.34\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740179816 -->\r\n",
       "<g id=\"node7\" class=\"node\">\r\n",
       "<title>2419740179816</title>\r\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"206.5,-249 105.5,-249 105.5,-228 206.5,-228 206.5,-249\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"156\" y=\"-235.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">AddmmBackward</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740179816&#45;&gt;2419740178360 -->\r\n",
       "<g id=\"edge6\" class=\"edge\">\r\n",
       "<title>2419740179816&#45;&gt;2419740178360</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M157.22,-227.92C158.11,-220.91 159.35,-211.14 160.46,-202.47\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"163.96,-202.7 161.75,-192.34 157.01,-201.82 163.96,-202.7\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740179648 -->\r\n",
       "<g id=\"node8\" class=\"node\">\r\n",
       "<title>2419740179648</title>\r\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"152.5,-306 59.5,-306 59.5,-285 152.5,-285 152.5,-306\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"106\" y=\"-292.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">hidden.0.bias (4)</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740179648&#45;&gt;2419740179816 -->\r\n",
       "<g id=\"edge7\" class=\"edge\">\r\n",
       "<title>2419740179648&#45;&gt;2419740179816</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M114.71,-284.92C121.72,-277.21 131.76,-266.16 140.18,-256.9\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"142.92,-259.09 147.05,-249.34 137.74,-254.39 142.92,-259.09\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740179760 -->\r\n",
       "<g id=\"node9\" class=\"node\">\r\n",
       "<title>2419740179760</title>\r\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"241.5,-306 170.5,-306 170.5,-285 241.5,-285 241.5,-306\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"206\" y=\"-292.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">TBackward</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740179760&#45;&gt;2419740179816 -->\r\n",
       "<g id=\"edge8\" class=\"edge\">\r\n",
       "<title>2419740179760&#45;&gt;2419740179816</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M197.29,-284.92C190.28,-277.21 180.24,-266.16 171.82,-256.9\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"174.26,-254.39 164.95,-249.34 169.08,-259.09 174.26,-254.39\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740180096 -->\r\n",
       "<g id=\"node10\" class=\"node\">\r\n",
       "<title>2419740180096</title>\r\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"264,-363 148,-363 148,-342 264,-342 264,-363\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"206\" y=\"-349.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">hidden.0.weight (4, 2)</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740180096&#45;&gt;2419740179760 -->\r\n",
       "<g id=\"edge9\" class=\"edge\">\r\n",
       "<title>2419740180096&#45;&gt;2419740179760</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M206,-341.92C206,-334.91 206,-325.14 206,-316.47\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"209.5,-316.34 206,-306.34 202.5,-316.34 209.5,-316.34\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740180320 -->\r\n",
       "<g id=\"node11\" class=\"node\">\r\n",
       "<title>2419740180320</title>\r\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"311.5,-192 240.5,-192 240.5,-171 311.5,-171 311.5,-192\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"276\" y=\"-178.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">TBackward</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740180320&#45;&gt;2419740178304 -->\r\n",
       "<g id=\"edge10\" class=\"edge\">\r\n",
       "<title>2419740180320&#45;&gt;2419740178304</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M256.32,-170.92C238.52,-162.26 212.09,-149.4 191.9,-139.57\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"193.36,-136.38 182.84,-135.16 190.3,-142.68 193.36,-136.38\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740179928 -->\r\n",
       "<g id=\"node12\" class=\"node\">\r\n",
       "<title>2419740179928</title>\r\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"341,-249 225,-249 225,-228 341,-228 341,-249\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"283\" y=\"-235.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">hidden.2.weight (3, 4)</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740179928&#45;&gt;2419740180320 -->\r\n",
       "<g id=\"edge11\" class=\"edge\">\r\n",
       "<title>2419740179928&#45;&gt;2419740180320</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M281.78,-227.92C280.89,-220.91 279.65,-211.14 278.54,-202.47\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"281.99,-201.82 277.25,-192.34 275.04,-202.7 281.99,-201.82\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740180264 -->\r\n",
       "<g id=\"node13\" class=\"node\">\r\n",
       "<title>2419740180264</title>\r\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"325.5,-78 254.5,-78 254.5,-57 325.5,-57 325.5,-78\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"290\" y=\"-64.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">TBackward</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740180264&#45;&gt;2419740177632 -->\r\n",
       "<g id=\"edge12\" class=\"edge\">\r\n",
       "<title>2419740180264&#45;&gt;2419740177632</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M269.62,-56.92C251.11,-48.22 223.58,-35.28 202.64,-25.43\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"204.08,-22.24 193.54,-21.16 201.1,-28.58 204.08,-22.24\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740176568 -->\r\n",
       "<g id=\"node14\" class=\"node\">\r\n",
       "<title>2419740176568</title>\r\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"366,-135 232,-135 232,-114 366,-114 366,-135\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"299\" y=\"-121.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">output_layer.weight (1, 3)</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740176568&#45;&gt;2419740180264 -->\r\n",
       "<g id=\"edge13\" class=\"edge\">\r\n",
       "<title>2419740176568&#45;&gt;2419740180264</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M297.43,-113.92C296.28,-106.91 294.69,-97.14 293.27,-88.47\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"296.68,-87.64 291.61,-78.34 289.77,-88.77 296.68,-87.64\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x23363c0ff98>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize computational graph\n",
    "x = torch.randn(1, num_features, device=device)\n",
    "torchviz.make_dot(model(x), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/nolan/64/re-enter-pincode.png\" style=\"height:50px;display:inline\"> Weights Initialization\n",
    "---\n",
    "* As we have learned, neural networks are trained using a stochastic optimization algorithm, such as Gradient Descent, RMSprop, Adam and etc...\n",
    "* Recall that these algorithms require initializing the parameters to some values. That is, they use randomness in order to find a good enough set of weights for the specific mapping function from inputs to outputs in your data that is being learned.\n",
    "* These algroithms require that the weights of the network are initialized to small random values (random, but close to zero). \n",
    "    * Randomness is also used during the search process in the **shuffling of the training dataset** prior to each epoch, which in turn results in differences in the gradient estimate for each batch.\n",
    "* Training deep models is a sufficiently difficult task that most algorithms are strongly affected by the choice of initialization (page 301, <a href=\"https://amzn.to/2H5wjfg\">Deep Learning</a>, 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Why Not Just Initialize With Zeros?\n",
    "---\n",
    "* We can use the same set of weights each time we train the network. For example, you could use the values of 0.0 for all weights.\n",
    "* In this case, the equations of the learning algorithm would fail to make any changes to the network weights, and the model will be **stuck**. \n",
    "    * It is important to note that the bias weight in each neuron is set to zero by default, not a small random value.\n",
    "* Specifically, neurons that are in the same hidden layer that is connected to the same inputs must have different weights for the learning algorithm to update the weights.\n",
    "* **Symmetry Breaking**: initial parameters need to “break symmetry” between different units. If two hidden units with the same activation function are connected to the same inputs, then these units must have different initial parameters (page 301, <a href=\"https://amzn.to/2H5wjfg\">Deep Learning</a>, 2016). \n",
    "    * Why? If they have the same initial parameters, then a deterministic learning algorithm applied to a deterministic cost and model will constantly update both of these units in the same way.\n",
    "* Note that when you **constant the seed**, you will initialize with the same weights each time. We do this when we want to get reproducible results (or in production)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recent Trend: Non-Random Initializations\n",
    "---\n",
    "* <a href=\"https://arxiv.org/abs/2007.01038\">Beyond Signal Propagation: Is Feature Diversity Necessary in Deep Neural Network Initialization?</a>, a deep network is constructed with identical features by initializing almost all the weights to 0. The architecture also enables perfect signal propagation and stable gradients, and achieves high accuracy on standard benchmarks, indicating that random, diverse initializations are not always necessary for training neural networks.\n",
    "* <a href=\"https://openreview.net/forum?id=1AxQpKmiTc\">ZerO Initialization: Initializing Neural Networks with only Zeros and Ones</a>, the random weight initialization is replaced with a fully deterministic initialization scheme which initializes the weights of networks with only zeros and ones (up to a normalization factor), based on identity and Hadamard transforms. They show promising results on various benchmarks, paving the way to simpler initializations schemes that work just as well as random initializations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/emoji/96/000000/on-arrow-emoji.png\" style=\"height:50px;display:inline\"> Types of Weight Initialization\n",
    "---\n",
    "* The initialization of the weights of neural networks is an active field of study as the careful initialization of the network can speed up the learning process.\n",
    "\n",
    "* There is no single best way to initialize the weights of a neural network.\n",
    "\n",
    "* We will review some of the popular initalization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Unifrom** - initialize with values drawn from the uniform distribution $\\mathcal{U}(a, b)$\n",
    "    * In PyTorch - `torch.nn.init.uniform_(tensor, a=0.0, b=1.0)`\n",
    "* **Normal** - initialize with values drawn from the normal distribution $\\mathcal{N}(\\text{mean}, \\text{std}^2)$\n",
    "    * In PyTorch - `torch.nn.init.normal_(tensor, mean=0.0, std=1.0)`\n",
    "* **Constant** - initialize with the value $val$.\n",
    "    * In PyTorch - `torch.nn.init.constant_(tensor, val)`\n",
    "* **Ones** - Initialize with the scalar value 1.\n",
    "    * In PyTorch - `torch.nn.init.ones_(tensor)`\n",
    "* **Zeros** - Initialize with the scalar value 0.\n",
    "    * In PyTorch - `torch.nn.init.zeros_(tensor)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Xavier (Glorot) Uniform** - Initialize with values according to the method described in *Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010)*, using a uniform distribution. The resulting tensor will have values sampled from $\\mathcal{U}(-a, a)$ where $$ a = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan}_{in} + \\text{fan}_{out}}} $$\n",
    "    * `fan_in` is the number of input units in the weight tensor and `fan_out` is the number of output units in the weight tensor\n",
    "    * In PyTorch - `torch.nn.init.xavier_uniform_(tensor, gain=1.0)`\n",
    "    \n",
    "* **Xavier (Glorot) Normal** - Initialize with values according to the method described in *Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010)*, using a normal distribution. The resulting tensor will have values sampled from $\\mathcal{N}(0,\\text{std}^2)$ where $$ \\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan}_{in} + \\text{fan}_{out}}} $$\n",
    "    * `fan_in` is the number of input units in the weight tensor and `fan_out` is the number of output units in the weight tensor\n",
    "    * In PyTorch - `torch.nn.init.xavier_normal_(tensor, gain=1.0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Kaiming (He) Uniform** - Initialize with values according to the method described in *Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015)*, using a uniform distribution. The resulting tensor will have values sampled from $\\mathcal{U}(-\\text{bound}, \\text{bound})$ where $$ \\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{\\text{fan-mode}}} $$\n",
    "    * In PyTorch - `torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')`\n",
    "    * `a` - the negative slope of the rectifier used after this layer (only used with `leaky_relu`)\n",
    "    * `fan_mode` – either `fan_in` (default) or `fan_out`. Choosing `fan_in` preserves the magnitude of the variance of the weights in the forward pass. Choosing `fan_out` preserves the magnitudes in the backwards pass.\n",
    "    * `nonlinearity` – the non-linear function (`nn.functional` name), recommended to use only with `relu` or `leaky_relu` (default).\n",
    "    \n",
    "* **Kaiming (He) Normal** - Initialize with values according to the method described in  *Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015)*, using a normal distribution. The resulting tensor will have values sampled from $\\mathcal{N}(0,\\text{std}^2)$ where $$ \\text{std} = \\frac{\\text{gain}}{\\sqrt{\\text{fan-mode}}} $$\n",
    "    * In PyTorch - `torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has default initializations schemes that usually work good. For example, `kaiming_uniform` is <a href=\"https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear\">the default initialization in PyTorch for `Linear` layers</a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch default initialization for linear layers\n",
    "def reset_parameters(module):\n",
    "    # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "    # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\n",
    "    # https://github.com/pytorch/pytorch/issues/57109\n",
    "    torch.nn.init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n",
    "    if self.bias is not None:\n",
    "        fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(module.weight)\n",
    "        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "        torch.nn.init.uniform_(module.bias, -bound, bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Interactive Demo\n",
    "---\n",
    "<a href=\"https://www.deeplearning.ai/ai-notes/initialization/\">Different Initializations Demo</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/olympic-torch.png\" style=\"height:50px;display:inline\"> Neural Network Weight Initialization with PyTorch\n",
    "---\n",
    "* As from PyTorch 1.0, **most layers are initialized using Kaiming Uniform method by default**.\n",
    "* Let's see how we change the initialization of a model.\n",
    "* <a href=\"https://pytorch.org/docs/stable/nn.init.html\">Official PyTorch initialization documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define hyper-parmeters and create our model\n",
    "num_features = 2\n",
    "output_dim = 1\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "num_epochs = 500\n",
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# loss criterion\n",
    "criterion = nn.MSELoss()\n",
    "# model\n",
    "model = HousePricesMLP(num_features, output_dim).to(device)\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HousePricesMLP(\n",
       "  (hidden): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=4, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=4, out_features=3, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (output_layer): Linear(in_features=3, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a different initialization for the model\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        torch.nn.init.xavier_normal_(m.weight, gain=1.0)\n",
    "model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to do that\n",
    "class HousePricesMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(HousePricesMLP, self).__init__()\n",
    "        self.hidden = nn.Sequential(nn.Linear(input_dim, 4),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(4, 3),\n",
    "                                    nn.ReLU())\n",
    "        self.output_layer = nn.Linear(3, output_dim)\n",
    "        # NEW: init weights here\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output_layer(self.hidden(x))\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_normal_(m.weight, gain=1.0)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 624.5885009765625\n",
      "epoch: 50 loss: 35.445608139038086\n",
      "epoch: 100 loss: 28.66716480255127\n",
      "epoch: 150 loss: 22.615984439849854\n",
      "epoch: 200 loss: 20.051589488983154\n",
      "epoch: 250 loss: 19.82143211364746\n",
      "epoch: 300 loss: 19.730319499969482\n",
      "epoch: 350 loss: 19.615483283996582\n",
      "epoch: 400 loss: 19.475394248962402\n",
      "epoch: 450 loss: 19.32789421081543\n",
      "test MSE error: 15.348824501037598\n"
     ]
    }
   ],
   "source": [
    "boston_tensor_train_dataloader = DataLoader(boston_tensor_train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# training loop for the model\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for features, targets in boston_tensor_train_dataloader:\n",
    "        # send data to device\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # forward pass\n",
    "        output = model(features)\n",
    "        # loss\n",
    "        loss = criterion(output.view(-1), targets)\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()  # clean the gradients from previous iteration\n",
    "        loss.backward()  # autograd backward to calculate gradients\n",
    "        optimizer.step()  # apply update to the weights\n",
    "        epoch_losses.append(loss.item())\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'epoch: {epoch} loss: {np.mean(epoch_losses)}')\n",
    "        \n",
    "# test error\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(torch.tensor(x_test, dtype=torch.float, device=device))\n",
    "    test_error = criterion(test_outputs.view(-1), torch.tensor(y_test, dtype=torch.float, device=device))\n",
    "print(f'test MSE error: {test_error.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/50/000000/alps.png\" style=\"height:50px;display:inline\"> Deep Double Descent\n",
    "---\n",
    "* Double Descent in ML algorithms training: performance first improves, then gets worse, and then improves again with increasing model size, data size, or training time.\n",
    "* This effect is often avoided through careful **regularization** or **early stopping**. \n",
    "    * While this behavior appears to be fairly universal, *we don’t yet fully understand why it happens*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"./assets/double_descent_1.png\" style=\"height:300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* It can be seen that as we increase the number of parameters in a model, the test error initially decreases, increases, and, just as the model is able to fit the train set, undergoes a second descent. This is different than what we saw when we talked about the bias-variance trade-off.\n",
    "* Double descent also occurs over **train epochs**. \n",
    "    * Surprisingly, it can lead to a regime where **more data hurts**, and training a deep network on a larger train set actually performs worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Model-wise Double Descent\n",
    "---\n",
    "* There is a regime where **bigger models are worse**.\n",
    "* The model-wise double descent phenomenon can lead to a regime where training on more data hurts.\n",
    "\n",
    "<center><img src=\"./assets/double_descent_model.svg\" style=\"height:300px\"></center>\n",
    "\n",
    "In the figure, the peak in test error occurs around the interpolation threshold, **when the models are just barely large enough to fit the train set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Sample-wise Non-monotonicity\n",
    "---\n",
    "* There is a regime where **more samples hurts**.\n",
    "\n",
    "<center><img src=\"./assets/double_descent_sample.svg\" style=\"height:300px\"></center>\n",
    "\n",
    "* In the figure, increasing the number of samples shifts the curve downwards towards lower test error. \n",
    "* However, since more samples require larger models to fit, increasing the number of samples also shifts the interpolation threshold (and peak in test error) to the right.\n",
    "* For intermediate model sizes (red arrows), these two effects combine, and training on 4.5x more samples actually hurts test performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Epoch-wise Double Descent\n",
    "---\n",
    "* There is a regime where **training longer reverses overfitting**.\n",
    "\n",
    "<center><img src=\"./assets/double_descent_epoch.png\" style=\"height:300px\"></center>\n",
    "<center><img src=\"./assets/double_descent_epoch_2.png\" style=\"height:300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The figures above show test and train error as a function of both model size and number of optimization steps.\n",
    "* For a given number of optimization steps (fixed y-coordinate), test and train error exhibit model-size double descent.\n",
    "* For a given model size (fixed x-coordinate), as training proceeds, **test and train error decreases, increases, and decreases again!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**In general, the peak of test error appears systematically when models are just barely able to fit the train set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/stop-sign.png\" style=\"height:50px;display:inline\"> Question time!\n",
    "---\n",
    "This question appeared in Spring 2021 Moed A. \n",
    "\n",
    "You have to design an MLP with the following input: DNA sequences of length $d$. The DNA is a sequence of bases, where each base can be one of 4 options: $(C, T, G, A)$. Thus, the input can be described as the following matrix: $$ X \\in \\mathcal{R}^{4 \\times d}, $$ where $X[j,i]$ denotes the measured value of base concentration of the $j^{th}$ base at location $i$.\n",
    "\n",
    "The network should output a **binary** classification $y \\in \\{-1, 1\\}$ for a specific property we wish to find. The network will be trained on samples $\\{X^{(n)}, y^{(n)} \\}_{n=1}^{N}$, with a **logistic loss function**.\n",
    "\n",
    "First, we will examine a network with 1 hidden layerof size $4 \\times d$ and a **LeakyReLU** activation $\\phi$: $$ f_w(X) = \\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{i=1}^d W_1[r, k,j, i]X[j, i] \\right),$$ where $w=\\{W_1, W_2\\}$ are the layers of the weight **tensors**. After training is done, the classification will be done with $\\text{sign}(f(X))$.\n",
    "\n",
    "1. Which invariances exist in the network's parameters?\n",
    "2. Now, we notice the fact that: the *direction* in which the DNA is scanned is arbitrary. Thus, if for two inputs $X, \\tilde{X}$: $$ \\forall i,j: \\: X[j,i] = \\tilde{X}[j, d-i+1], $$ then the two inputs are **equivalent** in their meaning. What constraints should we put on the network's parameters to improve the network's classification performance? Explain why using an **invariant hidden layer** is not optimal.\n",
    "3. After that, we now recall that the DNA bases come in pairs, and thus if for two inputs $X, \\tilde{X}$: $$ \\forall i,j : \\: X[j,i] = \\tilde{X}[(4-j)\\text{mod}4 + 1,i] = \\tilde{X}[5-j,i], $$ then the two inputs are **equivalent** in their meaning. What constraints should we put on the network's parameters to improve the network's classification performance?\n",
    "4. We now notice that the measurement process in noisy, each sample $X^{(n)}$ is in arbitrary scale, and thus if for two $X, \\tilde{X}$: $$ \\forall i,j: \\: X[j,i] = c\\tilde{X}[j,i], $$ for some constant $c>0$, then the two inputs are **equivalent** in their meaning.\n",
    "    * (a) For the given network, that **is already trained**, what is the effect of the scale $c$ on the classification result?\n",
    "    * (b) Can the arbitrary scale hurt the training process? Hint: think what happens to the gradient of each sample.\n",
    "    * (c) How can use this information to improve the classifier performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/checkmark.png\" style=\"height:50px;display:inline\"> Answer time!\n",
    "---\n",
    "<details>\n",
    "  <summary>Hint</summary>\n",
    "\n",
    "1. Consider the interactions of $W_1,W_2$ and the resulting symmetry.\n",
    "2. The condition is straightforward.\n",
    "3. Similar to section 2\n",
    "4. What is or is not affected by scale?\n",
    "    \n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Solution</summary>\n",
    "\n",
    "#### Section 1\n",
    "First, there is a symmetry for permutation in the hidden layer - for each pair of indices $(j,i),(r,k)$, swapping \n",
    "$$W_1[j,i,:,:]\\leftrightarrow W_1[r,k,:,:],\\quad W_2[j,i] \\leftrightarrow W_2[r,k] $$\n",
    "does not change the outputs (permutation invariant).\n",
    "\n",
    "There is also a symmetry for multiplying and dividing by a constant, so the network output is transformation invariant\n",
    "$$\\forall r,k, c>0,\\quad W_1[r,k,:,:]\\rightarrow cW_1[r,k,:,:],\\quad W_2[r,k] \\rightarrow \\frac{1}{c}W_2[r,k] $$\n",
    "\n",
    "#### Section 2\n",
    "Equivariance requires that applying the transform on both the input and output of $W_1$ does not change the tensor $W_1$, so \n",
    "$$\\forall i,j,r,k, \\quad W_1[r,k,:,:]=W_1[r,d+1-k,j,d+1-i]$$\n",
    "For the output to be invariant to transformation, we must have \n",
    "$$\\forall r,k, \\quad W_2[r,k] \\leftrightarrow W_2[r,d+1-k]$$\n",
    "One can think of the second layer as a layer that preserves an equivariant representation that is followed by an average pooling. That is, we can define a tensor $\\tilde{W}_2 \\in \\mathcal{R}^{4\\times d \\times 4 \\times d}$ such that: $$ \\forall i,j,k,r: \\:\\tilde{W}_2[j,i,r,k]=\\tilde{W}_2[j, d-i + 1, r, d-k + 1] $$ $$W_2[r, k] = \\frac{1}{4d}\\sum_{j=1}^4\\sum_{i=1}^d \\tilde{W}_2[j,i,r,k], $$ and $(*)$ is true. **Why equivariance and not invariance?** In class, we learned that we need the hidden layer to be equivariant, and only before the output we will be invariant because even if the class itself is invariant to the transformation it doesn't entail that the hidden features are necessary invariant to the transformation. Why? Let's look at the specific case of reversing the DNA order: let's assume that the hidden layer is invariant, then $$ \\forall i,j,k,r : W_1[k,r,j,i] = W_1[k,r,j,d+1-i], $$ which means we are giving the same weight to input $i$ and its complementary input $d+1-i$. Let's say there is an important feature in the form of the difference between two complementary inputs in every DNA, then we would not be able to detect it (it will be an eigenvector with eigenvalue 0 in $W_1$).\n",
    "\n",
    "#### Section 3\n",
    "Similar as the previous section, but for an additional transformation: $$ \\forall i,j,k,r: \\: W_1[k,r,j,i] = W_1[(4-r)\\text{mod}4 + 1, k, (4-j)\\text{mod}4 + 1, i] =  W_1[5-r, k, 5-j, i].$$ In addition, to constrain the network's output to be invariant to this transformation we demand: $$ \\forall k,r: W_2[r,k]=W_2[(4-r)\\text{mod}4 + 1, k]=W_2[5-r, k].$$\n",
    "\n",
    "#### Section 4\n",
    "a. There is no effect on classification, since $f(cX)=cf(X)$ and therefore $\\mathrm{sign}(f(cX))=\\mathrm{sign}(f(X))$.\n",
    "\n",
    "b. Both the cost function and its gradient are affected by $c$:\n",
    "$$\\ell(y^{(n)},f_w(cX^{(n)}))=\\log(1+\\mathrm{exp}(-cy^{(n)}f_w(X^{(n)})))$$\n",
    "$$\\nabla_{w}\\ell(y^{(n)},f_w(cX^{(n)}))=\\frac{\\mathrm{exp}(-cy^{(n)}f_w(X^{(n)}))}{1+\\mathrm{exp}(-cy^{(n)}f_w(X^{(n)}))}(-cy^{(n)}\\nabla_{w}f_w(X^{(n)}))=\\frac{-cy^{(n)}}{1+\\mathrm{exp}(cy^{(n)}f_w(X^{(n)}))}\\nabla_{w}f_w(X^{(n)})$$\n",
    "For the gradient, very small gradients for a very small $c$ and also for for samples with very large $c$ that are classified as true. On the other hand, we will get a very large gradient for samples that are classified as false for which $c$ is very large. Large arbitrary differences between the gradient's magnitudes can harm the training process.\n",
    "\n",
    "c. We can just normalize the input:\n",
    "$$f_w(X)=\\sum_{r=1}^{4}\\sum_{k=1}^{d}W_2[r,k]\\varphi\\left (\\sum_{j=1}^{4}\\sum_{i=1}^{d}W_1[r,k,j,i]\\frac{X[j,i]}{\\sqrt{\\sum_{j'=1}^{4}\\sum_{i'=1}^{d}X^2[j',i']}}\\right )$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/video-playlist.png\" style=\"height:50px;display:inline\"> Recommended Videos\n",
    "---\n",
    "#### <img src=\"https://img.icons8.com/cute-clipart/64/000000/warning-shield.png\" style=\"height:30px;display:inline\"> Warning!\n",
    "* These videos do not replace the lectures and tutorials.\n",
    "* Please use these to get a better understanding of the material, and not as an alternative to the written material.\n",
    "\n",
    "#### Video By Subject\n",
    "\n",
    "* Deep Learning - <a href=\"https://www.youtube.com/watch?v=kPXxbmBsFxs\">Machine Learning Lecture 35 \"Neural Networks / Deep Learning\" -Cornell CS4780</a>\n",
    "    * <a href=\"https://www.youtube.com/watch?v=zmu9wR2c7Z4\">Machine Learning Lecture 36 \"Neural Networks / Deep Learning Continued\" -Cornell CS4780</a>\n",
    "* Building a Network with PyTorch - <a href=\"https://www.youtube.com/watch?v=ixathu7U-LQ\">Deep Learning and Neural Networks with Python and Pytorch</a>\n",
    "* Weight Initialization - <a href=\"https://www.youtube.com/watch?v=m1gt7nxbB2k\">UC Berkeley - STAT 157- Stabilize Training - Weight Initialization</a>\n",
    "    * <a href=\"https://www.youtube.com/watch?v=tMjdQLylyGI\">Krish Naik - Various Weight Initialization Techniques in Neural Network</a>\n",
    "    * <a href=\"https://www.youtube.com/watch?v=s2coXdufOzE\">Weight Initialization in a Deep Network (C2W1L11)</a>\n",
    "* Deep Double Descent - <a href=\"https://www.youtube.com/watch?v=R29awq6jvUw\">Henry AI Labs - Deep Double Descent</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
    "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/\n",
    "* <a href=\"https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/\">Jason Brownlee - Why Initialize a Neural Network with Random Weights?</a>\n",
    "* <a href=\"https://openai.com/blog/deep-double-descent/\">OpenAI - Deep Double Descent</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
