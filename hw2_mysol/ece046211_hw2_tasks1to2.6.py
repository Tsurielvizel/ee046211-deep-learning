# -*- coding: utf-8 -*-
"""ece046211_hw2_mlp_cnn_students.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vcH5Vtx0b2T1nALCRJj9Sq6mJ_heXj2s

# <img src="https://img.icons8.com/bubbles/50/000000/mind-map.png" style="height:50px;display:inline"> ECE 046211 - Technion - Deep Learning
---

## HW2 - Multilayer NNs and Convolutional NNs
---

### <img src="https://img.icons8.com/clouds/96/000000/keyboard.png" style="height:50px;display:inline"> Keyboard Shortcuts
---
* Run current cell: **Ctrl + Enter**
* Run current cell and move to the next: **Shift + Enter**
* Show lines in a code cell: **Esc + L**
* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`
* New cell below: **Esc + B**
* Delete cell: **Esc + D, D** (two D's)

### <img src="https://img.icons8.com/bubbles/50/000000/information.png" style="height:50px;display:inline"> Students Information
---
* Fill in

|Name     |Campus Email| ID  |
|---------|--------------------------------|----------|
|Student 1| student_1@campus.technion.ac.il| 123456789|
|Student 2| student_2@campus.technion.ac.il| 987654321|

### <img src="https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png" style="height:50px;display:inline"> Submission Guidelines
---
* Maximal garde: 100.
* Submission only in **pairs**.
    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).
* **No handwritten submissions.** You can choose whether to answer in a Markdown cell in this notebook or attach a PDF with your answers.
* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>
* What you have to submit:
    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ece046211_hw2_id1_id2.ipynb`.
    * If you answered the questions in a different file you should submit a `.zip` file with the name `ece046211_hw2_id1_id2.zip` with content:
        * `ece046211_hw2_id1_id2.ipynb` - the code tasks.
        * `ece046211_hw2_id1_id2.pdf` - answers to questions.
    * No other file-types (`.py`, `.docx`...) will be accepted.
* Submission on the course website (Moodle).
* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers ("* some text here with Latex equations" -> "some text here with Latex equations").

### <img src="https://img.icons8.com/dusk/64/000000/online.png" style="height:50px;display:inline"> Working Online and Locally
---
* You can choose your working environment:
    1. `Jupyter Notebook`, **locally** with <a href="https://www.anaconda.com/distribution/">Anaconda</a> or **online** on <a href="https://colab.research.google.com/">Google Colab</a>
        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\rightarrow$ `Change Runtime Type` $\rightarrow$`GPU`.
    2. Python IDE such as <a href="https://www.jetbrains.com/pycharm/">PyCharm</a> or <a href="https://code.visualstudio.com/">Visual Studio Code</a>.
        * Both allow editing and running Jupyter Notebooks.

* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046211-deep-learning) to help you get everything installed.
* If you need any technical assistance, please go to our Piazza forum (`hw2` folder) and describe your problem (preferably with images).

### <img src="https://img.icons8.com/bubbles/50/000000/checklist.png" style="height:50px;display:inline"> Agenda
---

* [Part 1 - Theory](#-Part-1---Theory)
    * [Q1 - Generalization in A Teacher-Student Setup](#-Question-1--Generalization-in-A-Teacher-Student-Setup)
    * [Q2 - "Typical" Generalization in Multilayer Neural Networks](#-Question-2---"Typical"-Generalization-in-Multilayer-Neural-Networks)
    * [Q3 - Deep Double Descent](#-Question-3---Deep-Double-Descent)
    * [Q4 - Initialization](#-Question-4---Initialization)
    * [Q5 - Equivariance](#-Question-5---Equivariance)
    * [Q6 - VGG Architecture](#-Question-6--VGG-Architecture)
* [Part 2 - Code Assignments](#-Part-2---Code-Assignments)
    * [Task 1 - The Importance of Activation and Initialization](#-Task-1---The-Importance-of-Activation-and-Initialization)
    * [Task 2 - MLP-based Deep Classifer](#-Task-2---MLP-based-Deep-Classifer)
    * [Task 3 - Design a CNN](#-Task-3---Design-a-CNN)
* [Credits](#-Credits)

### <img src="https://img.icons8.com/cute-clipart/64/000000/ball-point-pen.png" style="height:50px;display:inline"> Part 1 - Theory
---
* You can choose whether to answser these straight in the notebook (Markdown + Latex) or use another editor (Word, LyX, Latex, Overleaf...) and submit an additional PDF file, **but no handwritten submissions**.
* You can attach additional figures (drawings, graphs,...) in a separate PDF file, just make sure to refer to them in your answers.

* $\large\LaTeX$ <a href="https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index">Cheat-Sheet</a> (to write equations)
    * <a href="http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf">Another Cheat-Sheet</a>

## <img src="https://img.icons8.com/clouds/100/000000/question-mark.png" style="height:50px;display:inline"> Question 1 -Generalization in A Teacher-Student Setup
---

Recall from lecture 4 the Bayes Risk $\mathcal{\overline{R}}(w)$: $$ \mathcal{\overline{R}}(w) \triangleq \mathbb{E}_{\epsilon \sim \mathcal{N}(0, \sigma_{\epsilon}^2I), w_{true} \sim \mathcal{N}(0,\frac{\sigma_w^2}{d}I) } \left[\mathcal{R}\right], $$ where, $$ \mathcal{R}(w_{\mu}) = ||w_{\mu}-w_{true}||^2 = ||(H_{\mu}^{-1}H-I)w_{true} + H_{\mu}^{-1}X^T\epsilon||^2 $$

Prove:

$$ \overline{\mathcal{R}}(w_{\mu}) = \sum_{i=1}^d \frac{(\sigma_w^2/d) \mu^2 + \sigma_{\epsilon}^2 \lambda_i}{(\lambda_i + \mu)^2} $$

Hints:
* $\mathbb{E} \left[\epsilon^TXH_{\mu}^{-1}H_{\mu}^{-1}X^T\epsilon \right] = \sum_{i,j}^N\mathbb{E}[\epsilon_i \epsilon_j] \left(XH_{\mu}^{-1} \right)_i\left(H_{\mu}^{-1}X^T \right)_j$

* $\mathbb{E}[\epsilon_i \epsilon_j] = \sigma_{\epsilon}^2 \delta_{ij}$

* $\sum_{i=1}^N \left(XH_{\mu}^{-1} \right)_i\left(H_{\mu}^{-1}X^T \right)_i = Tr\left[XH_{\mu}^{-2}X^T \right] $

## <img src="https://img.icons8.com/clouds/100/000000/question-mark.png" style="height:50px;display:inline"> Question 2 - "Typical" Generalization in Multilayer Neural Networks
---


We examine a "student" neural network  $f_{\mathbf{w}}\left(\mathbf{x}\right)$ with parameter vector $\mathbf{w}\in\mathbb{R}^{k}$ and input $\mathbf{x}\in\mathbb{R}^{d_{0}}$  used in a binary classification problem where the training set is $\mathcal{S}=\left\{ \mathbf{x}^{\left(n\right)}\right\} _{n=1}^{N}$ sampled i.i.d. from $P_{X}$, where the binary $\left(\pm1\right)$ labels are generated by a "teacher" neural network $f_{\mathbf{w}_{\star}}\left(\mathbf{x}\right)$ with the same architecture. To understand the "typical" generalization of the student, we examine the following "Guess and Check" algorithm to learn its weights: we randomly sample parameters vectors $\mathbf{w}_{1},\mathbf{w}_{2},...$ i.i.d.from $P_{W}$, in which each parameter is sampled independently from a uniform distribution over $Q=\left\{ -\left(q-1\right)/2,\dots,-1,0,1,\dots,\left(q-1\right)/2\right\} $ quantization levels, where $q=\left|Q\right|$ is an odd positive number (assume that teacher weights are also in $Q$). We do this until a stopping time $t$ in which we perfectly fit the dataset: $\forall n:f_{\mathbf{w}_{t}}\left(\mathbf{x}^{\left(n\right)}\right)=f_{\mathbf{w}_{\star}}\left(\mathbf{x}^{\left(n\right)}\right)$. We examine a two-layer neural network with $d_{1}$ hidden neurons
$$
f_{\mathbf{w}}\left(\mathbf{x}\right)=\mathrm{sign}\left(\mathbf{w}_{2}^{\top}\left[\mathbf{W}_{1}\mathbf{x}\right]_{+}\right)
$$
where $\left[\cdot\right]_{+}$ is the ReLU activation function, the teacher has at most $d_{1}^{\star}<d_{1}$ non-zero neurons (i.e., the other $d_{1}-d_{1}^{\star}$ hidden neurons in the teacher to have all the incoming and outgoing weights equal to zero). Each of the teacher's weights are also in $Q$.

1. Calculate the probability $P_{\mathbf{w}\sim P_{W}}\left(\mathbf{w}=\mathbf{w}_{\star}\right).$
2. Prove that
\begin{equation}
(1) \:\:p_{\star}\triangleq P_{\mathbf{w}\sim P_{W}}\left(\forall\mathbf{x}:f_{\mathbf{w}}\left(\mathbf{x}\right)=f_{\mathbf{w}_{\star}}\left(\mathbf{x}\right)\right)\geq q^{-d_{0}d_{1}^{*}-d_{1}}\,.
\end{equation}
3. Show that for any constant $T>0$, we can bound the probability of stopping time $t>T$ as
\begin{equation} (2) \:\:
\left\lfloor T\right\rfloor \leq\frac{\log P\left(t>T\right)}{\log\left(1-p_{\star}\right)}\,.
\end{equation}
4. Prove the generalization bound:
<br>
   **Theorem 1** *With probability* $\left(1-\eta\right)\left(1-\delta\right)$,
\begin{equation}(3) \:\:
\epsilon<\frac{\left(d_{0}d_{1}^{\star}+d_{1}\right)\log q+\log\frac{1}{\delta}+\log\log\frac{1}{\eta}}{N}
\end{equation}
**Hint**: Combine the results from previous sections, using the approximations $\left\lfloor T\right\rfloor \approx T$ and $\log\left(1-p_{\star}\right)\approx-p_{\star}$ (treat these approximations as exact), and the following basic generalization
 bound (which we learned in class):
<br>
     **Theorem 2** *For any* $f\in\left|\mathcal{F}\right|$ $f\in\left|\mathcal{F}\right|$, *with probability* $1-\delta$,
\begin{equation} (4) \:\:
\epsilon\triangleq\mathbb{P}_{\mathbf{x}}\left(f_{\mathbf{w}}\left(\mathbf{x}\right)\neq f_{\mathbf{w}_{\star}}\left(\mathbf{x}\right)\right)<\frac{\log\left|\mathcal{F}\right|+\log\frac{1}{\delta}}{N}\,.
\end{equation}
6. Is the bound in eq. $(3)$ better than the bound in eq. $(4)$ in which $\mathcal{F}=\left\{ f_{\mathbf{w}}:\mathbf{w}\in Q^{k}\right\} $  is the student hypothesis class (in which each parameter can have one of $q$ values)? Explain and ignore the (negligble) $\log\log\frac{1}{\eta}$ term.

## <img src="https://img.icons8.com/clouds/100/000000/question-mark.png" style="height:50px;display:inline"> Question 3 - Deep Double Descent
---

For the following plots:
1. Where is the critical point (the point of transition between the "Classical Regime" and "Modern Regime") of the deep double descent?
2. What type of double descent is shown (**look closely at the graph**)? Explain. There can be more than one correct answer.
    

a. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_transformer.PNG' style="height:300px">

b. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_resnet.PNG' style="height:400px">

c. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_intermediate.PNG' style="height:300px">

## <img src="https://img.icons8.com/clouds/100/000000/question-mark.png" style="height:50px;display:inline"> Question 4 - Initialization
---
Recall that in lecture 5 we were discussing how to calculate the initialization variance, and reached the conclusion that $$ \sigma_l =\frac{1}{\sqrt{\sum_j \mathbb{E} \left[\varphi^2(u_{l-1}[j])\right]}} $$
Show that for ReLU activation ($\varphi(z) = max(0,z)$), the optimal variance satisfies: $$ \sigma_l = \sqrt{\frac{2}{d_{l-1}}}$$

1. Under the assumption that the distribution of $W$ is symmetric ($\to$ the distribution of $u$ is symmetric).
2. Using the central limit theorem for large width.

Answer each section **separately** and assume the sections are independent.

All the notations are the same as in the lecture slides.

## <img src="https://img.icons8.com/clouds/100/000000/question-mark.png" style="height:50px;display:inline"> Question 5 - Equivariance
---

Recall from lecture 6:
A function $f: \mathbb{R}^d \to \mathbb{R}^d$ is equivariant if $f(\tau \cdot x) = \tau \cdot f(x)$ for all $\tau$.

Let $f_w(x) = \phi (Wx)$ where $\phi$ is a component-wise non-linearity and $W \in \mathbb{R}^{d\times d}$. Prove that $f_w:\mathbb{R}^d \to \mathbb{R}^d$ is equivariant to transformation family $H$ **if and only if**: $$ \forall \tau \in H, W[i, j] = W[\tau(i), \tau(j)] $$

* As in class, $\tau$ is an operator which re-arranges the terms in the vector it is operating on. $\tau(i)=j$ implies that component $i$ is mapped to component $j$. In addition, $\tau \cdot x$ means we use $\tau$ on $x$.
* Assume one-by-one activations (<a href="https://en.wikipedia.org/wiki/Injective_function">Injective functions/one-by-one</a>)

## <img src="https://img.icons8.com/clouds/100/000000/question-mark.png" style="height:50px;display:inline"> Question 6 - VGG Architecture
---

1. The VGG-11 CNN architecture consists of 11 convolution (CONV)/fully-connected (FC) layers (every CONV layer has the same padding and stride, every MAXPOOL layer is 2×2 and has padding of 0 and stride 2). Fill in the table. You need to **consider the bias**.


* CONV$M$-$N$: a convolutional layer of size $M \times M \times N$, where $M$ is the kernel size and $N$ is the number of filters. $stride=1, padding=1$.
* POOL2: $2 \times 2$ Max Pooling with $stride=2$
    * In case the input of the layer is odd, you should round down. For example, if the output of the layer should be $3.5 \times 3.5 \times 3$, you should round to $3 \times 3 \times 3$ (i.e., ignore the last column of the input image when performing MaxPooling).
* FC-N: a fully connected layer with $N$ neurons.


| Layer  | Output Dimension  | Number of Parameters (Weights) |
|---|---|---|
| INPUT  |  224x224x3 | 0  |
|  CONV3-64 | -  | -  |
| ReLU |  - | -  |
| POOL2|  - | -  |
|CONV3-128 | - | -|
|ReLU | - | -|
| POOL2|  - | -  |
|CONV3-256 | - | -|
|ReLU | - | -|
|CONV3-256 | - | -|
|ReLU | - | -|
| POOL2|  - | -  |
|CONV3-512 | - | -|
|ReLU | - | -|
|CONV3-512 | - | -|
|ReLU | - | -|
| POOL2|  - | -  |
|CONV3-512 | - | -|
|ReLU | - | -|
|CONV3-512 | - | -|
|ReLU | - | -|
| POOL2|  - | -  |
| FC-4096|  - | -  |
| FC-4096|  - | -  |
| FC-1000|  - | -  |
| SOFTMAX|  - | -  |

2. What is the total number of parameters? (use a calculator for this one)
3. What percentage of the weights are found in the fully-connected layers?

### <img src="https://img.icons8.com/officel/80/000000/code.png" style="height:50px;display:inline"> Part 2 - Code Assignments
---
* You must write your code in this notebook and save it with the output of all of the code cells.
* Additional text can be added in Markdown cells.
* You can use any other IDE you like (PyCharm, VSCode...) to write/debug your code, but for the submission you must copy it to this notebook, run the code and save the notebook with the output.

#### Tips
---
1. Uniformly distributed tensors - `torch.Tensor(dim1, dim2, ...,dimN).uniform_(-1, 1)`
2. Separation to **validation set** in PyTorch - <a href="https://gist.github.com/MattKleinsmith/5226a94bad5dd12ed0b871aed98cb123">See example here</a>.
"""

# Commented out IPython magic to ensure Python compatibility.
# imports for the practice (you can add more if you need)
import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import torchvision
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
# %matplotlib ipympl
# %matplotlib inline

seed = 211
np.random.seed(seed)
torch.manual_seed(seed)

"""### <img src="https://img.icons8.com/color/48/000000/code.png" style="height:50px;display:inline"> Task 1 - The Importance of Activation and Initialization
---
In this task, we are going to use $x \in \mathcal{R}^{512}$ and simple neural network that outputs $f(x) \in \mathcal{R}^{512}$. The network will have 100 layers with 512 units in each layer.

1. We initialize the weights from a unit normal distribution. Run the following code cell and explain what happens. Add a short piece of code that locates when it happens (hint: use `torch.isnan()`). **Print** the layer number.
2. We can demonstrate that at a given layer, the matrix product of inputs $x$ and weight matrix $a$ that is initialized from a standard normal distribution will, on average, have a standard deviation very close to the square root of the number of input connections. For our example, with 512 dimensions, show that for 10,000 multiplications of $a$ and $x$, the empirical standard deviation is similar to the square root of the number of input connections. Use the unbiased version: $$ \hat{std} = \sqrt{\frac{\sum_{i=1}^{10000}\frac{1}{N}\sum_{j=1}^N y^2}{10000}}, $$ where $y=ax$ and $N$ is the number of input connections. **Print** the mean, std and the square root of the number of input connections.
3. For the code from 1, normalize the weight initialization by the square root of the input connections. How does that change the outcome? **Print** the mean and std after the modification.
4. Add a `tanh()` activation after each layer for the code from 1. **Print** the mean and std after the modification. Explain the result.
5. Xavier initialization sets a layer’s weights to values chosen from a random uniform distribution that’s bounded between $$\pm \sqrt{\frac{6}{n_i + n_{i+1}}}$$ where $n_i$ is the number of incoming network connections, or “fan-in,” to the layer, and $ n_{i+1}$ is the number of outgoing network connections from that layer, also known as the “fan-out”. Glorot and Bengio believed that Xavier weight initialization would maintain the variance of activations and back-propagated gradients all the way up or down the layers of a network and demonstrated that networks initialized with Xavier achieved substantially quicker convergence and higher accuracy. Implement **Xavier Uniform** as `xavier_init(fan_in, fan_out)`, a function that returns a tensor initialized according to **Xavier Uniform**. Use it on the simple network from 1 with `tanh` activation. **Print** the mean and std after the modification.
6. If you try to replace the `tanh` activation with `relu` activation in section 5, you will see very different results. Xavier strives to acheive activation outputs of each layer to have a mean of 0 and a standard deviation around 1, on average. When using a ReLU activation, a single layer will, on average have standard deviation that’s very close to the square root of the number of input connections, **divided by the square root of two** ($\sqrt{\frac{512}{2}}$ in our example). **Kaiming He et. al.** proposed an initialization scheme that’s tailored for deep neural nets that use these kinds of asymmetric, non-linear activations. Implement **Kaiming Normal** as `kaiming_init(fan_in, fan_out)`, a function that returns a tensor initialized according to **Kaiming Normal** (use `fan_in` mode). Use it on the simple network from 1 with `relu` activation. **Print** the mean and std after the modification. What happens when you use Xavier with RelU activation?

<div dir="rtl">
<h4>
תשובה:
בגלל שאנחנו מכפילים כל פעם את
$x$ ב-$a$,
$x$ כל פעם גדל יותר ויותר (בערכו המוחלט)
בצורה אקספוננציאלית
(כי לפחות חלק מהמספרים גדולים מ-1)
ואז זה מספרים בגודל שה-**Python** מייצג בתור **NaN**
</h1>
</div>
"""

x = torch.randn(512)
first_time = False
for i in range(100):
    a = torch.randn(512, 512)
    x = a @ x
    if(first_time==False):
     if(torch.isnan(x.mean()) or torch.isnan(x.std())):
         print("first iteration, x mean is nan is {first_iter} ".format(first_iter=i))
         first_time=True

print(x.mean(), x.std())

N = 512
summ =0
y_sum =0
for i in range(10000):
  if(i%1000==0):
    print(i)
  x=torch.randn(N)
  a=torch.randn(N,N)
  y=a@x
  y_sum = y_sum + y
  for j in range(N):
    summ+=y[j]**2/N

std = (summ/(10000))**(1/2)
y_mean = y_sum / 10000
print('the formula std is {thesum}'.format(thesum = std))
print("the squre root of N is ",N**(1/2))
print("the mean of all y's is {mean}, the std of all y's mean is {std}".format(mean=y_mean.mean(),std=y.std()))

"""כמו שניתן לראות, הנוסחה יצאה כמעט שווה לשורש  גודל הקלט, והממוצע קרוב ל0."""

#3
x = torch.randn(512)
for i in range(100):
    a = torch.randn(512, 512)/(512**(1/2))
    x = a @ x

print("x mean is", x.mean(), "x std is",  x.std())

"""עכשיו הפלט הוא עם תוחלת 0 בערך וסטיית תקן 1 בערך, כמו שרצינו"""

#4
x = torch.randn(512)
for i in range(100):
    a = torch.randn(512, 512)
    x = a @ x
    x = torch.tanh(x)

print("x mean is", x.mean(), "x std is",  x.std())

"""כיוון שטנגנס היפרבולי מגביל את הערכים בין מינוס 1 ל1 , הערכים לא יתפוצצו

"""

#5
def xavier_init_correct(fan_in, fan_out):
    limit = (6 / (fan_in + fan_out))**0.5
    return torch.empty(fan_out, fan_in).uniform_(-limit, limit)
x = torch.randn(512)
for i in range(100):
    a = xavier_init_correct(512, 512)
    x = a @ x
    x = torch.tanh(x)

print("x mean is", x.mean(), "x std is",  x.std())

#6
def xavier_init_correct(fan_in, fan_out):
    limit = (6 / (fan_in + fan_out))**0.5
    return torch.empty(fan_out, fan_in).uniform_(-limit, limit)
x = torch.randn(512)
for i in range(100):
    a = xavier_init_correct(512, 512)
    x = a @ x
    x = torch.relu(x)

print("x mean is", x.mean(), "x std is",  x.std())



"""when you use Xavier with RelU activation the mean and std tend to zero"""

def kaiming_init(fan_in, fan_out):
    factor = (2/fan_in)**0.5
    return factor*torch.randn(fan_out, fan_in)
x = torch.randn(512)
for i in range(100):
    a = kaiming_init(512, 512)
    x = a @ x
    x = torch.relu(x)

print("with Kaiming Normal x mean is", x.mean(), "x std is",  x.std())

"""### <img src="https://img.icons8.com/color/48/000000/code.png" style="height:50px;display:inline"> Task 2 - MLP-based Deep Classifer
---
In this task you are going to design and train your first neural network for classification.

For this task, we will use the "<a href="https://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope">MAGIC Gamma Telescope Data Set"</a>. Cherenkov gamma telescope observes high energy gamma rays, taking advantage of the radiation emitted by charged particles produced inside the electromagnetic showers initiated by the gammas, and developing in the atmosphere. This Cherenkov radiation (of visible to UV wavelengths) leaks through the atmosphere and gets recorded in the detector, allowing reconstruction of the shower parameters. The available information consists of pulses left by the incoming Cherenkov photons on the photomultiplier tubes, arranged in a plane, the camera.

Depending on the energy of the primary gamma, a total of few hundreds to some 10000 Cherenkov photons get collected, in patterns (called the shower image), allowing to discriminate statistically those caused by primary gammas (**signal**) from the images of hadronic showers initiated by cosmic rays in the upper atmosphere (**background**).

Our data has 10 features and 2 classes (signal and background).

1. Load the MAGIC dataset sored in `magic04.data` and display the first 5 features (just run the cell).
2. Separate the data to train, validation and test, reserve 10% of the data for validation and 20% for test.
3. Perform pre-processing steps of your choice and convert the class label from `str` to `int` (for example, `y_train = np.array([0 if y_train[i] == 'g' else 1 for i in range(len(y_train))]).astype(np.int)`).
4. Train a Logistic Regression model from `sklearn` as a baseline for our neural network (only for this section use both the train and validation sets for training the classifier). **Print the test accuracy**.
5. Convert the `numpy` arrays to `torch` tensors with `TensorDataset` as done in the tutorial.
6. Design a **MLP** to classify the data. Optimize the hyper-parameters of your model using the accuracy on the validation set, and when you are satisfied with the model train it on both the train and validation sets and evaluate it on the test set. **You need to reach at least 85% accuracy on the test set, and 87% for a full grade**.
    * You have a free choice of architecture, optimizer, learning scheduler, initialization, regularization and activations.
    * The loss criterion is binary cross entropy: `nn.BCEWithLogitsLoss()` (performs `sigmoid` for you) or `nn.BCELoss` (you need to apply `sigmoid` on the network output yourself).
    * In a Markdown block, write down the chosen architectures and all the hyper-parameters.
        * Make sure to describe any design choice that you used to improve the performance (e.g. if you used a certain regularization or layer, mention it and describe why you think it helped).
    * **Plot** the loss curves (and any oter statistic you want) as a function of epochs/iterations. **Print** the final performance.
    * **Print** the test accuracy.
7. Pick **2** initializations of your choosing and change the initialization of the linear layers and re-train the model (with the same optimal hyper-parameters you found). You can pick an initialization of your choosing from : https://pytorch.org/docs/stable/nn.init.html . See example below how to use. **Print** the change in accuracy for both changes (you should end up with 3 results - original, `init 1` and `init 2`).
"""

# imports for the practice (you can add more if you need)
import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import torchvision
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
# %matplotlib ipympl
# %matplotlib inline

seed = 211
np.random.seed(seed)
torch.manual_seed(seed)

# loading the data
col_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym',  'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']
feature_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym',  'fM3Long', 'fM3Trans', 'fAlpha', 'fDist']
#data = pd.read_csv("./magic04.data", names=col_names)
data = pd.read_csv("/content/magic04.data", names=col_names)
X = data[feature_names]
Y = data['class']
data.head()

# separate to train, test
x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,random_state=36)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.125,random_state=18)

#3 pre-processing and converting labels to integers

y_train_np = np.array([0 if y == 'g' else 1 for y in y_train]).astype(int)
y_val_np = np.array([0 if y == 'g' else 1 for y in y_val]).astype(int)
y_test_np = np.array([0 if y == 'g' else 1 for y in y_test]).astype(int)

#4 training a Logistic Regression baseline - complete the code with your variables
logstic_model = LogisticRegression(solver='lbfgs', max_iter=1000) #
X_train_val_lr = pd.concat([x_train, x_val])
y_train_val_lr = np.concatenate([y_train_np, y_val_np])

y_pred = logstic_model.fit(X_train_val_lr, y_train_val_lr).predict(X_train_val_lr)
print("Logistic Regression Model accuracy =" , logstic_model.score(x_test, y_test_np))

#5
#  create TensorDataset from numpy arrays

# make the numpy Tensors

x_train_tensor = torch.tensor(x_train.values, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train_np, dtype=torch.long)

x_val_tensor = torch.tensor(x_val.values, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val_np, dtype=torch.long)

x_test_tensor = torch.tensor(x_test.values, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test_np, dtype=torch.long)

# making TensorDataset
train_dataset = TensorDataset(x_train_tensor, y_train_tensor)
val_dataset = TensorDataset(x_val_tensor, y_val_tensor)
test_dataset = TensorDataset(x_test_tensor, y_test_tensor)

batch_size = 64

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

#6
# Define the MLP  model

class MLPClassifier(nn.Module):
    def __init__(self, input_size=10, dropout_rate=0.2):
        super(MLPClassifier, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_size, 256),
            nn.BatchNorm1d(256),
            nn.LeakyReLU(0.01),
            nn.Dropout(dropout_rate),

            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.LeakyReLU(0.01),
            nn.Dropout(dropout_rate),

            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.LeakyReLU(0.01),
            nn.Dropout(dropout_rate),

            nn.Linear(64, 1)
        )

    def forward(self, x):
        return self.model(x)

# Function to train the model
def train_model(model, train_loader, val_loader, epochs=50, lr=0.001, weight_decay=0.0):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    criterion = nn.BCEWithLogitsLoss()

    train_losses, val_losses, val_accuracies = [], [], []
    best_val_accuracy_for_this_run = 0.0  # Tracks best accuracy for THIS specific training run
    best_model_state_dict_local = None    # Will store the weights of the model at its best validation accuracy

    for epoch in range(epochs):
        model.train()
        epoch_loss = 0
        for xb, yb in train_loader:
            optimizer.zero_grad()
            out = model(xb).squeeze(1)
            loss = criterion(out, yb.float())
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        train_losses.append(epoch_loss / len(train_loader))

        if val_loader:
            model.eval()
            val_loss, correct, total = 0, 0, 0
            with torch.no_grad():
                for xb, yb in val_loader:
                    out = model(xb).squeeze(1)
                    loss = criterion(out, yb.float())
                    val_loss += loss.item()

                    preds = (torch.sigmoid(out) > 0.5).long()
                    correct += (preds == yb).sum().item()
                    total += yb.size(0)

            current_val_accuracy = correct / total
            val_losses.append(val_loss / len(val_loader))
            val_accuracies.append(current_val_accuracy)

            if current_val_accuracy > best_val_accuracy_for_this_run:
                best_val_accuracy_for_this_run = current_val_accuracy
                best_model_state_dict_local = model.state_dict()

            print(f"Epoch {epoch+1}/{epochs} - Train Loss: {train_losses[-1]:.4f} | "
                  f"Val Loss: {val_losses[-1]:.4f} | Val Acc: {val_accuracies[-1]*100:.2f}%")
        else:
            print(f"Epoch {epoch+1}/{epochs} - Train Loss: {train_losses[-1]:.4f}")

    if best_model_state_dict_local:
        model.load_state_dict(best_model_state_dict_local)
        print("Loaded best model weights for this run (from epoch with best validation accuracy).")
    else:
        print("Warning: No improvement in validation accuracy from initial state. Returning model from last epoch.")

    print(f"Best Validation Accuracy reported by train_model: {best_val_accuracy_for_this_run*100:.2f}%")
    return train_losses, val_losses, val_accuracies, best_val_accuracy_for_this_run

model = MLPClassifier()
train_losses, val_losses, val_accuracies, _ = train_model(model, train_loader, val_loader, epochs=100, lr=0.001,weight_decay=0.001)

plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.plot(val_accuracies, label='Validation Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.title("Loss and Accuracy per Epoch")
plt.show()

# Function to evaluate the model
def evaluate(model, test_loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for xb, yb in test_loader:
            out = model(xb).squeeze(1)
            preds = (torch.sigmoid(out) > 0.5).long()
            correct += (preds == yb).sum().item()
            total += yb.size(0)
    acc = correct / total
    print(f"Test Accuracy: {acc*100:.2f}%")
    return acc

# model, hyoer-paramerters and training
"""
Your Code Here - add as many blocks as you wish
"""

# model, hyoer-paramerters and training
# Hyperparameter Optimization
print("\n--- Starting Hyperparameter Optimization ---")

best_overall_accuracy = 0.0
best_hyperparams = {}
best_model_state_dict = None

learning_rates = [0.0007, 0.0008]

epochs_options = [150]

dropout_rates = [0.15, 0.3]

#weight_decay_options = [0.0, 0.0001, 0.0005, 0.001]
weight_decay_options = [0.0]

# Add a new nested loop for weight_decay
for lr in learning_rates:
    for epochs in epochs_options:
        for dropout in dropout_rates:
            for wd in weight_decay_options: # <-- NEW LOOP HERE
                print(f"\nTesting with LR: {lr}, Epochs: {epochs}, Dropout: {dropout}, Weight Decay: {wd}")
                temp_model = MLPClassifier(dropout_rate=dropout)

                _, _, _, current_best_val_accuracy = train_model(temp_model, train_loader, val_loader,
                                                                  epochs=epochs, lr=lr, weight_decay=wd)

                if current_best_val_accuracy > best_overall_accuracy:
                    best_overall_accuracy = current_best_val_accuracy
                    best_hyperparams = {'lr': lr, 'epochs': epochs, 'dropout': dropout, 'weight_decay': wd}
                    best_model_state_dict = temp_model.state_dict()

print("\n--- Hyperparameter Optimization Complete ---")
print(f"Best Hyperparameters found: {best_hyperparams}")
print(f"Best Validation Accuracy during search: {best_overall_accuracy*100:.2f}%")

# Combine train and validation data for final model training
full_train_tensor = torch.tensor(np.vstack([x_train.values, x_val.values]), dtype=torch.float32)
full_label_tensor = torch.tensor(np.concatenate([y_train_np, y_val_np]), dtype=torch.long)

full_dataset = TensorDataset(full_train_tensor, full_label_tensor)
full_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)

# Train the final model on combined Train + Validation set with best hyperparameters
print("\n--- Training final model on combined Train + Validation set with best hyperparameters ---")
final_model = MLPClassifier(dropout_rate=best_hyperparams['dropout']) # No .to(device)
train_losses_final, _, _, _ = train_model(final_model, full_loader, val_loader=None, epochs=best_hyperparams['epochs'], lr=best_hyperparams['lr'],weight_decay=best_hyperparams['weight_decay'])
final_test_accuracy = evaluate(final_model, test_loader)

# example of weight initialization
import torch.nn as nn
class MyModel(nn.Module):
    def __init__(self, parmaeters):
        super(MyModel, self).__init__()
        # model definitions/blocks
        # ...
        # custom initialization
        self.init_weights()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                # pick initialzation: https://pytorch.org/docs/stable/nn.init.html
                # examples
                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                # nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu', a=math.sqrt(5))
                # nn.init.normal_(m.weight, 0, 0.005)
                # don't forget the bias term (m.bias)

    def forward(self, x):
        # ops on x
        # ...
        # output = f(x)
        return output

"""### **ארכיטקטורה והיפר-פרמטרים של רשת נוירונים (MLP)**

**ארכיטקטורה (MLPClassifier):**
המודל הוא רשת עצבית רב-שכבתית (MLP) הכוללת את השכבות הבאות:
1.  **שכבה לינארית קלט (Input Layer):** מקבלת 10 פיצ'רים.
2.  **שכבה נסתרת ראשונה:** שכבה לינארית עם 64 יחידות פלט.
    * **פונקציית אקטיבציה:** ReLU (Rectified Linear Unit) - נבחרה בזכות פשטותה ויעילותה במניעת בעיית הגרדיאנטים הנעלמים (vanishing gradients), ומאפשרת למידה מהירה יותר.
    * **Dropout:** שכבת Dropout עם קצב משתנה (נבחר באופטימיזציית היפר-פרמטרים) - נבחרה כדי למנוע התאמת יתר (overfitting). היא "מכבה" אחוז מסוים של נוירונים באופן אקראי במהלך כל איטרציה של אימון, ובכך מאלצת את הרשת ללמוד ייצוגים יציבים יותר.
3.  **שכבה נסתרת שנייה:** שכבה לינארית עם 32 יחידות פלט.
    * **פונקציית אקטיבציה:** ReLU.
    * **Dropout:** שכבת Dropout עם קצב משתנה (נבחר באופטימיזציית היפר-פרמטרים) - כמו בשכבה הראשונה, גם כאן למניעת התאמת יתר.
4.  **שכבת פלט (Output Layer):** שכבה לינארית עם יחידת פלט אחת, המתאימה למשימת סיווג בינארי.

**היפר-פרמטרים:**
היפר-פרמטרים אלו נבחרו באמצעות תהליך אופטימיזציית היפר-פרמטרים על בסיס ביצועי סט הולידציה:
* **קצב למידה (Learning Rate):** הערך האופטימלי שנמצא.
* **אופטימייזר:** Adam - נבחר בזכות יכולותיו להתמודד עם קצבי למידה משתנים באופן דינמי ולספק ביצועים טובים במגוון רחב של משימות.
* **פונקציית הפסד (Loss Criterion):** `nn.BCEWithLogitsLoss()` - זוהי פונקציית הפסד המתאימה לסיווג בינארי, והיא משלבת את פונקציית הסיגמואיד (sigmoid) בתוכה, מה שהופך את חישוב הלוג-הסתברותי ליציב יותר מספרית.
* **מספר אפוכות (Epochs):** הערך האופטימלי שנמצא.
* **קצב Dropout:** הערך האופטימלי שנמצא.
* **גודל אצווה (Batch Size):** 64 - נבחר כערך סטנדרטי המספק איזון בין יציבות אימון למהירות.

**שיקולי עיצוב לשיפור ביצועים:**
* **Dropout (ניתן לכיוון):** שימוש בשכבות Dropout סייע למנוע התאמת יתר (overfitting) לנתוני האימון. היכולת לכייל את קצב ה-Dropout אפשרה למצוא את האיזון הטוב ביותר בין למידה להכללה.
* **פונקציית אקטיבציה ReLU:** בחירה ב-ReLU מעל פונקציות כמו Sigmoid או Tanh סייעה בהאצת תהליך האימון ובמניעת בעיות גרדיאנט נעלם, מה שהיה קריטי למודל עמוק יחסית.
* **אופטימייזר Adam:** Adam הוא אופטימייזר חזק ויעיל שמתאים את קצב הלמידה עבור כל פרמטר באופן אוטומטי, מה שמקל על תהליך האופטימיזציה ומשיג התכנסות טובה יותר.
"""

plt.plot(train_losses_final, label='Train Loss') # **שינוי: שימוש בנתוני האימון הסופיים.**
plt.plot(val_losses_final, label='Validation Loss') # **שינוי: שימוש בנתוני האימון הסופיים.**
plt.plot(val_accuracies_final, label='Validation Accuracy') # **שינוי: שימוש בנתוני האימון הסופיים.**
plt.xlabel('Epoch')
plt.legend()
plt.title("Loss and Accuracy per Epoch (Final Model)") # **שינוי: כותרת הגרף.**
plt.show()

print("\n--- Starting Initialization Comparison (Task 7) ---")

# Define the optimal hyperparameters found from the previous optimization step
# **שינוי: שימוש בערכים שנמצאו באופטימיזציה.**
optimal_lr = best_hyperparams['lr']
optimal_epochs = best_hyperparams['epochs']
optimal_dropout = best_hyperparams['dropout']

# Helper function to reset model weights for a new training run
def reset_weights(m):
    # This function resets the weights of linear layers to their default PyTorch initialization
    if isinstance(m, nn.Linear):
        m.reset_parameters()

# Helper function to apply specific initialization
def apply_custom_init(m, init_type):
    if isinstance(m, nn.Linear):
        if init_type == 'xavier_uniform':
            nn.init.xavier_uniform_(m.weight)
        elif init_type == 'kaiming_normal':
            # 'relu' nonlinearity is specified for Kaiming initialization
            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
        # biases are usually initialized to zeros, we focus on weights for this task.


# --- Run 1: Default PyTorch Initialization ---
print("\n--- Running with Default PyTorch Initialization ---")
model_default_init = MLPClassifier(dropout_rate=optimal_dropout) # **שינוי: שימוש בקצב dropout אופטימלי.**
# Ensure a fresh start with default initialization
model_default_init.apply(reset_weights)
# Train the model on the full combined dataset (train + val)
# **שינוי: שימוש בהיפר-פרמטרים האופטימליים.**
_, _, _, _ = train_model(model_default_init, full_loader, val_loader=None, epochs=optimal_epochs, lr=optimal_lr)
# Evaluate on test set
acc_default = evaluate(model_default_init, test_loader)
print(f"Test Accuracy (Default Initialization): {acc_default*100:.2f}%")


# --- Run 2: Xavier Uniform Initialization ---
print("\n--- Running with Xavier Uniform Initialization ---")
model_xavier_init = MLPClassifier(dropout_rate=optimal_dropout) # **שינוי: שימוש בקצב dropout אופטימלי.**
# Apply Xavier Uniform initialization
model_xavier_init.apply(lambda m: apply_custom_init(m, 'xavier_uniform'))
# Train the model on the full combined dataset (train + val)
# **שינוי: שימוש בהיפר-פרמטרים האופטימליים.**
_, _, _, _ = train_model(model_xavier_init, full_loader, val_loader=None, epochs=optimal_epochs, lr=optimal_lr)
# Evaluate on test set
acc_xavier = evaluate(model_xavier_init, test_loader)
print(f"Test Accuracy (Xavier Uniform Initialization): {acc_xavier*100:.2f}%")


# --- Run 3: Kaiming Normal Initialization ---
print("\n--- Running with Kaiming Normal Initialization ---")
model_kaiming_init = MLPClassifier(dropout_rate=optimal_dropout) # **שינוי: שימוש בקצב dropout אופטימלי.**
# Apply Kaiming Normal initialization
model_kaiming_init.apply(lambda m: apply_custom_init(m, 'kaiming_normal'))
# Train the model on the full combined dataset (train + val)
# **שינוי: שימוש בהיפר-פרמטרים האופטימליים.**
_, _, _, _ = train_model(model_kaiming_init, full_loader, val_loader=None, epochs=optimal_epochs, lr=optimal_lr)
# Evaluate on test set
acc_kaiming = evaluate(model_kaiming_init, test_loader)
print(f"Test Accuracy (Kaiming Normal Initialization): {acc_kaiming*100:.2f}%")


print("\n--- Initialization Comparison Results ---")
print(f"Original Test Accuracy (from the final optimized model): {final_test_accuracy*100:.2f}%")
print(f"Test Accuracy (Default Initialization): {acc_default*100:.2f}%")
print(f"Test Accuracy (Xavier Uniform Initialization): {acc_xavier*100:.2f}%")
print(f"Test Accuracy (Kaiming Normal Initialization): {acc_kaiming*100:.2f}%")

"""### <img src="https://img.icons8.com/color/48/000000/code.png" style="height:50px;display:inline"> Task 3 - Design a CNN
---
In this task you are going to design a deep convolutional neural network to classify 10 classes from Imagenet (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute) - **The Imagenette Dataset**.

* 10 classes, 1 for each object.
* 9469 images for training and 3925 for testing (70/30 separation).
* We will use a downscaled version where the images are resized to $64\times 64$ resolution.

<center><img src="https://storage.googleapis.com/tfds-data/visualization/fig/imagenette-160px-v2-1.0.0.png" style="height:300px"></center>

1. Load the the Imagenette dataset with PyTorch using `torchvision.datasets.Imagenette(
    root='./datasets', split='train', size='160px', download=True, transform=transform_train)`, where `split` is either `'train'` or `'val'`, you can read more here: https://pytorch.org/vision/main/generated/torchvision.datasets.Imagenette.html#torchvision.datasets.Imagenette . Use the `transform` parameter to resize the images to $64 \times 64$ (for train, validation and test) and convert the data to tensors, e.g.,
   
   <code>transform_test=transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor(),])</code>
    
   Display 5 images from the train set.

   <a href="https://gist.github.com/kevinzakka/d33bf8d6c7f06a9d8c76d97a7879f5cb">Train, Validation and Test Split for torchvision Datasets</a>
3. Design a Convolutional Neural Network (CNN) to classify classes from the images.
    * You are **not allowed** to use `BatchNorm` in your architecture, but can use any other normalization (`GroupNorm`, `LayerNorm`, and etc..).
    * Describe the chosen architecture, how many layers? What activations did you choose? What are the filter sizes? Did you use fully-connected layers (if you did, explain their sizes)?
    * What is the input dimension? What is the output dimension?
    * Calculate the number of parameters (weights) in the network. What is the model size in MegaBytes (MB)? (see the convolution tutorial). **Print** these numbers.
4. Train the classifier (preferably on a GPU - use Colab for this part if you don't have a GPU).
    * **DO NOT USE ANY IMAGE AUGMENTATIONS IN THIS PART** (You can still use `Normalize` if you wish, but no cropping, flipping and etc...).
    * You are not allowed to use pre-trained models (i.e., no transfer learning, only learning from scratch).
    * Describe the hyper-parameters of the model (batch size, epochs, optimizer, learning rate, scheduler....). How did you tune your model? Did you use a validation set to tune the model?
    * What is the final accuracy on the test set? **Print** it.
        * You need to reach at least 73% accuracy in this section, and 78% for maximum points in section 5.
    * **Plot** the loss curves (and any other statistic you calculate) as a function of epochs/iterations.
6. For the trained classifier, what is the accuracy on the test set when each test image is added a small noise $a=(0.05, 0.01, 0.005)$: $$ \text{image} + a \times \mathcal{N}(0, 1).$$ **Print** the result for each value of $a$.
7. Retrain the classifier, but this time use data augementations of your choosing. Briefly explain what augmentation you chose and how it works. Did the test accuracy improve? **Print** the result.
    * You can use transformations available in `torchvision.transforms` as shown in the tutorial.
    * You are welcome to use <a href="https://kornia.github.io/">`kornia`</a> for the augmentations (**2 points bonus**, maximal grade is still 100).
    * **Plot** the loss curves (and any other statistic you want) as a function of epochs/iterations.

Your Code Here - add as many blocks as you wish

## <img src="https://img.icons8.com/dusk/64/000000/prize.png" style="height:50px;display:inline"> Credits
---
* Icons made by <a href="https://www.flaticon.com/authors/becris" title="Becris">Becris</a> from <a href="https://www.flaticon.com/" title="Flaticon">www.flaticon.com</a>
* Icons from <a href="https://icons8.com/">Icons8.com</a> - https://icons8.com
* Datasets from <a href="https://www.kaggle.com/">Kaggle</a> - https://www.kaggle.com/
"""